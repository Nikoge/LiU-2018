---
title: "lab1_block2"
author: "Thijs Quast"
date: "30-11-2018"
output: pdf_document
toc: yes
---

\newpage
# Question 1 Ensemble Methods

```{r}
# Loading packages and importing files ####
library(mboost)
library(randomForest)
library(ggplot2)
sp <- read.csv2("spambase.csv", header = FALSE, sep = ",", stringsAsFactors = FALSE)
num_sp <- data.frame(data.matrix(sp))
num_sp$V58 <- factor(num_sp$V58)
```

```{r}
# shuffling data and dividing into train and test ####
n <- dim(num_sp)[1]
ncol <- dim(num_sp)[2]
set.seed(1234567890)
id <- sample(1:n, floor(n*(2/3)))
train <- num_sp[id,]
test <- num_sp[-id,]
```

```{r}
# Adaboost
ntree <- c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
error <- c()

for (i in seq(from = 10, to = 100, by = 10)){
bb <- blackboost(V58 ~., data = train, control = boost_control(mstop = i), family = AdaExp())
bb_predict <- predict(bb, newdata = test, type = c("class"))
confusion_bb <- table(test$V58, bb_predict)
miss_class_bb <- (confusion_bb[1,2] + confusion_bb[2,1])/nrow(test)
error[(i/10)] <- miss_class_bb
}

error_df <- data.frame(cbind(ntree, error))
```

```{r}
# Random forest ####
ntree_rf <- c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
error_rf <- c()

for (i in seq(from = 10, to = 100, by = 10)){
rf <- randomForest(V58 ~., data = train, ntree= 10)
rf_predict <- predict(rf, newdata = test, type = c("class"))
confusion_rf <- table(test$V58, rf_predict)
miss_class_rf <- (confusion_rf[1,2] + confusion_rf[2,1])/nrow(test)
error_rf[i/10] <- miss_class_rf
}

error_df_rf <- data.frame(cbind(ntree_rf, error_rf))
```

```{r}

df <- cbind(error_df, error_df_rf)
df <- df[, -3]

plot_final <- ggplot(df, aes(ntree)) + 
  geom_line(aes(y=error, color = "Adaboost")) +
  geom_line(aes(y=error_rf, color = "Random forest"))

plot_final <- plot_final + ggtitle("Error rate vs number of trees")
plot_final
```

The error rate for the AdaBoost model are clearly going down when the number of trees increases. Finally the model arrives at an error rate below 7% when 100 trees are included in the model. For the randomforest the pattern is less obvious, the error rate seems to go up and down as the number of trees in the model increases. 50 trees result in the lowest error rate. This error rate is also lower than the error rate produced by the best Adaboost model (100 trees). Therefore, for this spam classification, a randomforest with 50 trees seems to be most suitable.

# Question 2 Mixture Models
```{r}

my_own_em <- function(K){
# 2 - Mixture Models ####
set.seed(1234567890)

max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data

true_pi <- vector(length = K) # true mixing coefficients
true_mu <- matrix(nrow=K, ncol=D) # true conditional distributions
true_pi=c(rep(1/3, K))


if (K == 2){
  true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
  true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
  plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
  points(true_mu[2,], type="o", col="red")
}else if (K == 3){
  true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
  true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
  true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
  plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
  points(true_mu[2,], type="o", col="red")
  points(true_mu[3,], type="o", col="green")
}else{
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
true_mu[4,]=c(0.3,0.5,0.5,0.7,0.5,0.5,0.5,0.5,0.4,0.5)
plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
points(true_mu[4,], type="o", col="yellow")
}


# Producing the training data
for(n in 1:N) {
  k <- sample(1:K,1,prob=true_pi)
  for(d in 1:D) {
    x[n,d] <- rbinom(1,1,true_mu[k,d])
  }
}

 # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
  mu[k,] <- runif(D,0.49,0.51)
}
pi 
mu
for(it in 1:max_it) {
  if (K == 2){
    plot(mu[1,], type="o", col="blue", ylim=c(0,1))
    points(mu[2,], type="o", col="red")
  }else if (K == 3){
    plot(mu[1,], type="o", col="blue", ylim=c(0,1))
    points(mu[2,], type="o", col="red")
    points(mu[3,], type="o", col="green")
  }else{
    plot(mu[1,], type="o", col="blue", ylim=c(0,1))
    points(mu[2,], type="o", col="red")
    points(mu[3,], type="o", col="green")
    points(mu[4,], type="o", col="yellow")
  }
  Sys.sleep(0.5)
  # E-step: Computation of the fractional component assignments
  m <- matrix(NA, nrow = 1000, ncol = k)
  
  #Here I create the Bernouilli probabilities, lecture 1b, slide 7. I use 3 loops to do it for the three distributions
  # not very efficient, but it works.
  for (j in 1:k){
    for(each in 1:nrow(x)){
      row <- x[each,]
      vec <- c()
      for (i in 1:10) {
        a <- mu[j,i]^row[i]
        b <- a * ((1-mu[j,i])^(1-row[i]))
        vec[i] <- b
        c <- prod(vec)
      }
      m[each, j] <- c
    }
  }

  # Here I create a empty matrix, to store all values for the numerator of the formula on the bottom of
  # slide 9, lecture 1b.
  m2 <- matrix(NA, ncol = k, nrow = 1000)
  
  # m2 stores all the values for the numerator of the formula on the bottom of slide 9, lecture 1b.
  for (i in 1:1000){
    a <- pi * m[i,]
    m2[i,] <- a
  }
  
  # Sum m2 to get the denominator of the formula on the bottom of slide 9, lecture 1b.
  m2_sum <- rowSums(m2)
  m_final <- m2 / m2_sum

  #Log likelihood computation.
  ll <- matrix(nrow = 1000, ncol = K)
  for (j in 1:K){
    for (i in 1:1000){
      ll[i, j] <- sum(((x[i,] * log(mu[j,])) + (1 - x[i,])*log(1-mu[j,])))
    }
  }
  
  ll <- ll + pi
  llnew <- m_final * ll
  llik[it] <- sum(rowSums(llnew))
  
  cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
  flush.console()
  # Stop if the lok likelihood has not changed significantly
  if (it != 1){
  if (abs(llik[it] - llik[it-1]) < min_change) {break}
  }
  #M-step: ML parameter estimation from the data and fractional component assignments
  
  # Create the numerator for pi, slide 9, lecture 1b.
  numerator_pi <- colSums(m_final)
  
  # Create new values for pi, stored in the vector pi_new
  pi_new <- numerator_pi / N
  pi_new
  mnew <- matrix(NA, nrow = 1000, ncol = 10)
  mu_new <- matrix(NA, nrow = K, ncol = 10)
  
  for (j in 1:k){
    for (i in 1:1000){
      row <- x[i,] * m_final[i,j]
      mnew[i,] <- row
    }
    mnewsum <- colSums(mnew)/numerator_pi[j]
    mu_new[j,] <- mnewsum
  }

  # Now, to create the iterations, I have to run the code again and again, and specifying mu as new the new values
  # created for mu. Same goes for the other variables.
  mu <- mu_new
  pi <- pi_new
}
z <- m_final
output1 <- pi
output2 <- mu
output3 <- plot(llik[1:it], type="o")
z
result <- list(c(output1, output2, output3))
return(result)
}
```

```{r}
my_own_em(2)
```

```{r}
my_own_em(3)
```

```{r}
my_own_em(4)
```

# Appendix 
```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```


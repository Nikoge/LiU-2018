---
title: "machine learning(732A99) lab1"
author: "Anubhav Dikshit(anudi287)"
date: "26 November 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 1

## Loading The Libraries
```{r, message=FALSE, echo = FALSE}

if (!require("pacman")) install.packages("pacman")
pacman::p_load(xlsx, glmnet, MASS, jtools, huxtable, ggplot2, 
               ggthemes, gridExtra, ROCR, broom, caret, e1071,
               kknn, tidyr, dplyr)

options("jtools-digits" = 2, scipen = 999)

```

## Loading Input files
```{r}
spam_data <- read.xlsx("spambase.xlsx", sheetName = "spambase_data")
spam_data$Spam <- as.factor(spam_data$Spam)

tecator_data <- read.xlsx("tecator.xlsx", sheetName = "data")

```

## 1.1 Import the data into R and divide it into training and test sets (50%/50%) by using the following code
```{r}
set.seed(12345)

n =  NROW(spam_data)
id = sample(1:n, floor(n*0.5))
train = spam_data[id,]
test = spam_data[-id,]
```

## 1.2 Use logistic regression (functions glm(), predict()) to classify the training and test data by the classification principles
```{r, results='hide', message=FALSE, warning=FALSE, echo=TRUE, include=TRUE}

min.model = glm(Spam ~ 1, family=binomial, data=train)
biggest <- formula(glm(Spam ~., family=binomial, data=train))

full.model <- glm(Spam ~., family=binomial, data=train)
step.model <- step(min.model, direction='forward', scope=biggest)
summary(step.model)
```

### Manual Feature Selection
```{r, warning=FALSE}
best_model <- glm(formula = Spam ~ Word35 + Word46 + Word42 + Word44 + Word33 + 
    Word45 + Word39 + Word48 + Word30 + Word43 + Word37 + 
    Word36 + Word31, family = binomial, data = train)

#export_summs(step.model, best_model, 
#model.names = c("Model using Step","Model Manually Tunned"))
```

###  Prediction for probability greater than 50% and 90%
```{r}
# prediction
train$prediction_prob <- predict(best_model, newdata = train, type = "response")
test$prediction_prob <- predict(best_model, newdata = test , type = "response")

train$prediction_class_50 <- ifelse(train$prediction_prob > 0.50, 1, 0)
test$prediction_class_50 <- ifelse(test$prediction_prob > 0.50, 1, 0)

train$prediction_class_90 <- ifelse(train$prediction_prob > 0.90, 1, 0)
test$prediction_class_90 <- ifelse(test$prediction_prob > 0.90, 1, 0)

```

### Assessing the Model
```{r}
# plots
ggplot(train, aes(prediction_prob, color = Spam)) + 
geom_density(size = 1) + ggtitle("Training Set's Predicted Score for 50% cutoff") + 
  scale_color_economist(name = "data", labels = c("negative", "positive")) + 
  theme_economist()


ggplot(test, aes(prediction_prob, color = Spam)) + 
geom_density(size = 1) + ggtitle("Test Set's Predicted Score for 50% cutoff") + 
  scale_color_economist(name = "data", labels = c("negative", "positive")) +  
  theme_economist()

```

## 1.2 Assessing the Fit on train dataset for 50%
```{r}
#confusion table
conf_train <- table(train$Spam, train$prediction_class_50)
names(dimnames(conf_train)) <- c("Actual Train", "Predicted Train")
confusionMatrix(conf_train)

conf_test <- table(test$Spam, test$prediction_class_50)
names(dimnames(conf_test)) <- c("Actual Test", "Predicted Test")
confusionMatrix(conf_test)
```
Analysis: Distribution of the prediction score grouped by known outcome given that our model's final objective is to classify new instances into one of two categories (spam vs. non-spam). We will want the model to give high scores to positive instances (1: spam) and low scores (0 : not spam) otherwise.Ideally you want the distribution of scores to be separated, with the score of the negative instances to be on the left and the score of the positive instance to be on the right.

From the confusion matrix it is apparent that Accuracy on train and test dataset when cutoff=50% is about 83%.


## 1.3 Assessing the Fit on train dataset for 90%
```{r}

#confusion table
conf_train1 <- table(train$Spam, train$prediction_class_90)
names(dimnames(conf_train1)) <- c("Actual Train", "Predicted Train")
conf_train1

conf_test1 <- table(test$Spam, test$prediction_class_90)
names(dimnames(conf_test1)) <- c("Actual Test", "Predicted Test")
conf_test1

```

Analysis: Strange, the model only predicts one class!! We know that the prediction of a logistic regression model is a probability, thus in order to use it as a classifier, we'll have to choose a cutoff value, or threshold (cutoff). Where scores above this value will classified as positive, those below as negative. Lets us find this optimum value.

### Choosing the best cutoff for test
```{r}

cutoffs <- seq(from = 0.05, to = 0.95, by = 0.05)
accuracy <- NULL

for (i in seq_along(cutoffs)){
    prediction <- ifelse(test$prediction_prob >= cutoffs[i], 1, 0) #Predicting for cut-off

    accuracy <- c(accuracy,length(which(test$Spam == prediction))/length(prediction)*100)}

cutoff_data <- as.data.frame(cbind(cutoffs, accuracy))

ggplot(data = cutoff_data, aes(x = cutoffs, y = accuracy)) + 
  geom_line() + 
  ggtitle("Cutoff vs. Accuracy for Test Dataset")

```

Analysis: Our small detour suggests that the cutoff value of 50% was the best for our purpose and going higher than this leads to worse results, at 0.8 and above the accuracy drastically reduces which is what we see when we make cutoff as 0.9.

From the confusion matrix it is evident that the model becomes a trivial model(predicts all cases as one class) and thus the prediction is no better than tossing a coin. This should be the absoutely the worst case that we should avoid.

## 1.4 Use standard classifier kknn() with K=30 from package kknn, report the the misclassification rates for the training and test data and compare the results with step 1.2.
```{r}
knn_model30 <- train.kknn(Spam ~ Word35 + Word46 + Word42 + Word44 + Word33 + 
    Word45 + Word39 + Word48 + Word30 + Word43 + Word37 + 
    Word36 + Word31, data = train, kmax = 30)

train$knn_prediction_class <- predict(knn_model30, train)
test$knn_prediction_class <- predict(knn_model30, test)

conf_train2 <- table(train$Spam, train$knn_prediction_class)
names(dimnames(conf_train2)) <- c("Actual Train", "Predicted Train")
confusionMatrix(conf_train2)

conf_test2 <- table(test$Spam, test$knn_prediction_class)
names(dimnames(conf_test2)) <- c("Actual Test", "Predicted Test")
confusionMatrix(conf_test2)
```

Analysis: Using KKNN with K = 30, increased our trainning accuracy to 90%, however using trainning error/accuracyis a bad 83%


## 1.5 Repeat step 4 for K=1 and compare the results with step 4. What effect does the decrease of K lead to and why?
```{r}
knn_model1 <- train.kknn(Spam ~ Word35 + Word46 + Word42 + Word44 + Word33 + 
    Word45 + Word39 + Word48 + Word30 + Word43 + Word37 + 
    Word36 + Word31, data = train, kmax = 1)

train$knn_prediction_class <- predict(knn_model1, train)
test$knn_prediction_class <- predict(knn_model1, test)

conf_train2 <- table(train$Spam, train$knn_prediction_class)
names(dimnames(conf_train2)) <- c("Actual Train", "Predicted Train")
confusionMatrix(conf_train2)

conf_test2 <- table(test$Spam, test$knn_prediction_class)
names(dimnames(conf_test2)) <- c("Actual Test", "Predicted Test")
confusionMatrix(conf_test2)
```
Analysis: 


# Assignment 2 Feature selection by cross-validation in a linear model

## 2.1 Implement an R function that performs feature selection (best subset selection) in linear regression by using k-fold cross-validation without using any specialized function like lm() (use only basic R functions)

```{r}

subset_function <- function(X,Y,N){

X = swiss[,1:5]
Y = swiss[,6:6]
N = 5

df <- cbind(X,Y)
  
 temp <- NULL
for(i in 1:NCOL(X)){
combs <- as.data.frame(gtools::combinations(NCOL(X), r=i, v=colnames(X), repeats.allowed=FALSE))
combs <- tidyr::unite(combs, "formula", sep = "+")
temp <- rbind(combs, temp)
}  
  
set.seed(12345)
df2 <- df[sample(nrow(df)),]
df2$k_fold <- sample(N, size = nrow(df), replace = TRUE)
 
result <- NULL 


for (j in 1:NROW(temp))
{
  for(i in 1:N){
    
train = df2[df2$k_fold != i,]
test = df2[df2$k_fold == i,]

model_forumla = paste("Y ~ ", temp[j,],sep = "")

model <- lm(formula = model_forumla, data = train)
predicted <- predict(model, newdata = test)

RMSE <- sqrt(mean((predicted - test$Y)^2))
data <- cbind(i,temp[j,], RMSE)
result <- rbind(data, result)

}
}

result <- as.data.frame(result)

colnames(result) <- c("kfold", "variables", "rmse")

result$rmse <- as.numeric(result$rmse)
result$no_variables <- nchar(as.character(result$variables)) - nchar(gsub('\\+', "", result$variables)) + 1

variable_performance <- result %>% group_by(kfold, no_variables) %>% summarise(RMSE = mean(rmse, na.rm = TRUE))


myplot <- ggplot(data = result, aes(x = avriables, y = rmse, color=kfold)) + 
geom_bar(stat="identity") + coord_flip()

return(myplot)  
}

temp <- subset_function(X = swiss[,1:5], Y = swiss[,6], N = 5)

```



# Apendix
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```
---
title: "machine learning(732A99) lab1"
author: "Anubhav Dikshit(anudi287)"
date: "26 November 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 1

## Loading The Libraries
```{r, message=FALSE, echo = FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(xlsx, glmnet, MASS, jtools, huxtable, ggplot2, 
               ggthemes, gridExtra, ROCR, broom, caret, e1071,
               kknn, tidyr, dplyr,reshape2, glmnet)

options("jtools-digits" = 2, scipen = 999)

```

## Loading Input files
```{r}
spam_data <- read.xlsx("spambase.xlsx", sheetName = "spambase_data")
spam_data$Spam <- as.factor(spam_data$Spam)

tecator_data <- read.xlsx("tecator.xlsx", sheetName = "data")
tecator_data <- tecator_data[,2:NCOL(tecator_data)] # removing sample column
```

## 1.1 Import the data into R and divide it into training and test sets (50%/50%) by using the following code
```{r}
set.seed(12345)

n =  NROW(spam_data)
id = sample(1:n, floor(n*0.5))
train = spam_data[id,]
test = spam_data[-id,]
```

## 1.2 Use logistic regression (functions glm(), predict()) to classify the training and test data by the classification principles

### Manual Feature Selection
```{r, warning=FALSE}
best_model <- glm(formula = Spam ~., family = binomial, data = train)
summ(best_model)
```

###  Prediction for probability greater than 50% and 90%
```{r}
# prediction
train$prediction_prob <- predict(best_model, newdata = train, type = "response")
test$prediction_prob <- predict(best_model, newdata = test , type = "response")

train$prediction_class_50 <- ifelse(train$prediction_prob > 0.50, 1, 0)
test$prediction_class_50 <- ifelse(test$prediction_prob > 0.50, 1, 0)

train$prediction_class_90 <- ifelse(train$prediction_prob > 0.90, 1, 0)
test$prediction_class_90 <- ifelse(test$prediction_prob > 0.90, 1, 0)
```

### Assessing the Model
```{r}
# plots
ggplot(train, aes(prediction_prob, color = Spam)) + 
geom_density(size = 1) + ggtitle("Training Set's Predicted Score for 50% cutoff") + 
  scale_color_economist(name = "data", labels = c("negative", "positive")) + 
  theme_economist()


ggplot(test, aes(prediction_prob, color = Spam)) + 
geom_density(size = 1) + ggtitle("Test Set's Predicted Score for 50% cutoff") + 
  scale_color_economist(name = "data", labels = c("negative", "positive")) +  
  theme_economist()

```

## 1.2 Assessing the Fit on train dataset for 50%
```{r}
#confusion table
conf_train <- table(train$Spam, train$prediction_class_50)
names(dimnames(conf_train)) <- c("Actual Train", "Predicted Train")
confusionMatrix(conf_train)

conf_test <- table(test$Spam, test$prediction_class_50)
names(dimnames(conf_test)) <- c("Actual Test", "Predicted Test")
confusionMatrix(conf_test)
```

Analysis: Distribution of the prediction score grouped by known outcome given that our model's final objective is to classify new instances into one of two categories (spam vs. non-spam). We will want the model to give high scores to positive instances (1: spam) and low scores (0 : not spam) otherwise.Ideally you want the distribution of scores to be separated, with the score of the negative instances to be on the left and the score of the positive instance to be on the right.

From the confusion matrix it is apparent that Accuracy on train and test dataset when cutoff=50% is about ~84% for train and ~82% for test, thus the misclassification rate is ~16 and ~18% for the train and test dataset.

## 1.3 Assessing the Fit on train dataset for 90%
```{r}

#confusion table
conf_train1 <- table(train$Spam, train$prediction_class_90)
names(dimnames(conf_train1)) <- c("Actual Train", "Predicted Train")
conf_train1

conf_test1 <- table(test$Spam, test$prediction_class_90)
names(dimnames(conf_test1)) <- c("Actual Test", "Predicted Test")
conf_test1

```

Analysis: Strange, the model almost only predicts one class!! We know that the prediction of a logistic regression model is a probability, thus in order to use it as a classifier, we'll have to choose a cutoff value, or threshold (cutoff). Where scores above this value will classified as positive, those below as negative. Lets us find this optimum value.

### Choosing the best cutoff for test
```{r}

cutoffs <- seq(from = 0.05, to = 0.95, by = 0.05)
accuracy <- NULL

for (i in seq_along(cutoffs)){
    prediction <- ifelse(test$prediction_prob >= cutoffs[i], 1, 0) #Predicting for cut-off

    accuracy <- c(accuracy,length(which(test$Spam == prediction))/length(prediction)*100)}

cutoff_data <- as.data.frame(cbind(cutoffs, accuracy))

ggplot(data = cutoff_data, aes(x = cutoffs, y = accuracy)) + 
  geom_line() + 
  ggtitle("Cutoff vs. Accuracy for Test Dataset")

```

Analysis: Our small detour suggests that the cutoff value of ~60% was the best for our purpose and going higher than this leads to worse results, at 70% and above the accuracy drastically reduces which is what we see when we make cutoff as 90%.

From the confusion matrix it is evident that the model becomes a trivial model(predicts all cases as one class) and thus the prediction is worse than just tossing a coin. This should be the absoutely the worst case that we should avoid.

The missclassication rate is about 31% for both the trainning dataset and test dataset.

## 1.4 Use standard classifier kknn() with K=30 from package kknn, report the the misclassification rates for the training and test data and compare the results with step 1.2.

```{r}
knn_model30 <- train.kknn(Spam ~., data = train, kmax = 30)

train$knn_prediction_class <- predict(knn_model30, train)
test$knn_prediction_class <- predict(knn_model30, test)

conf_train2 <- table(train$Spam, train$knn_prediction_class)
names(dimnames(conf_train2)) <- c("Actual Train", "Predicted Train")
confusionMatrix(conf_train2)

conf_test2 <- table(test$Spam, test$knn_prediction_class)
names(dimnames(conf_test2)) <- c("Actual Test", "Predicted Test")
confusionMatrix(conf_test2)
```

Analysis: Using KKNN with K = 30, we increased our trainning accuracy to 86%, thus misclassification is 14%, however for the test dataset misclassification rate is about ~20%.

Thus compared to using logisitc model the misclassification error for the trainning dataset decreased by 2% to 14%, while for the test dataset the misclassification error increased by 2% to 20%. 

## 1.5 Repeat step 4 for K=1 and compare the results with step 4. What effect does the decrease of K lead to and why?
```{r}
knn_model1 <- train.kknn(Spam ~., data = train, kmax = 1)

train$knn_prediction_class <- predict(knn_model1, train)
test$knn_prediction_class <- predict(knn_model1, test)

conf_train2 <- table(train$Spam, train$knn_prediction_class)
names(dimnames(conf_train2)) <- c("Actual Train", "Predicted Train")
confusionMatrix(conf_train2)

conf_test2 <- table(test$Spam, test$knn_prediction_class)
names(dimnames(conf_test2)) <- c("Actual Test", "Predicted Test")
confusionMatrix(conf_test2)
```

Analysis: Using KKNN with K = 1, we increased our trainning accuracy to 100%, thus misclassification is 0%, however for the test dataset accuracy is ~75% thus misclassification rate is about ~25%, thus we improved on the trainning accuracy but did bad on the test case, thus this is an example of overfitting leading to more varience.

Explaination: The KKNN works in the following way,  An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Thus K=1, makes the seperation boundary to be very complex and locally optimised (lots of local clusters), while as K goes higher, the decision boundary becomes more linear/simple.

# Assignment 2 Feature selection by cross-validation in a linear model

## 2.1 Implement an R function that performs feature selection (best subset selection) in linear regression by using k-fold cross-validation without using any specialized function like lm() (use only basic R functions)

```{r}

subset_function <- function(X,Y,N){

# X = swiss[,1:5]
# Y = swiss[,6:6]
# N = 5

df <- cbind(X,Y)
temp <- NULL
final <- NULL
 
for(i in 1:NCOL(X)){
combs <- as.data.frame(gtools::combinations(NCOL(X), r=i, v=colnames(X), repeats.allowed=FALSE))
combs <- tidyr::unite(combs, "formula", sep = ",")
temp <- rbind(combs, temp)
}  
  
set.seed(12345)
df2 <- df[sample(nrow(df)),]
df2$k_fold <- sample(N, size = nrow(df), replace = TRUE)
 
result <- NULL 

for (j in 1:NROW(temp))
{
  for(i in 1:N){

train = df2[df2$k_fold != i,]
test = df2[df2$k_fold == i,]
              
vec <- temp[j,]
train_trimmed = lapply(strsplit(as.character(vec), ","), function(x) train[x])[[1]]
test_trimmed = lapply(strsplit(as.character(vec), ","), function(x) test[x])[[1]]

y_train = train[,c("Y"), drop = FALSE]
y_test = test[,c("Y"), drop = FALSE]

train_trimmed = as.matrix(train_trimmed)
test_trimmed = as.matrix(test_trimmed)
y_test = as.matrix(y_test) 
y_train = as.matrix(y_train) 

t_train =  as.matrix(t(train_trimmed)) 
t_test =  as.matrix(t(test_trimmed)) 

betas = solve(t_train %*% train_trimmed) %*% (t_train %*% y_train)

train_trimmed = as.data.frame(train_trimmed)
test_trimmed = as.data.frame(test_trimmed)

train_trimmed$type = "train"
test_trimmed$type = "test"
final <- rbind(train_trimmed, test_trimmed)


y_hat_val = as.matrix(final[,1:(ncol(final)-1)]) %*% betas
mse = (Y - y_hat_val)^ 2

data <- cbind(i, vec, mse, type = final$type)
result <- rbind(data, result)
     
}
}

result <- as.data.frame(result)

colnames(result) <- c("kfold", "variables", "mse", "type")

result$mse <- as.numeric(result$mse)
result$no_variables <- nchar(as.character(result$variables)) - nchar(gsub('\\,', "", result$variables)) + 1

variable_performance <- result %>% group_by(kfold, no_variables) %>% 
  summarise(MSE = mean(mse, na.rm = TRUE))

myplot <- ggplot(data = variable_performance, aes(x = no_variables, y = MSE, color=kfold)) + 
geom_line() + ggtitle("Plot of MSE vs. Number of variables by folds")

myplot2 <- ggplot(data = result, aes(x = variables, y = mse, color=kfold)) + 
geom_bar(stat="identity") + ggtitle("Plot of RMSE vs. Features by folds") + coord_flip() 

return(list(myplot, myplot2))
}
```


## 2.2 Test your function on data set swiss available in the standard R repository:
```{r}
subset_function(X = swiss[,1:5], Y = swiss[,6], N = 5)
```
Analysis:

# Assignment 3 Linear regression and regularization

## 3.1 Import data to R and create a plot of Moisture versus Protein. Do you think that these data are described well by linear model.
```{r}
ggplot(data = tecator_data, aes(x = Protein, y = Moisture)) + 
  geom_point() + 
  geom_smooth( method = 'loess') +
  ggtitle("Plot of Moisture vs. Protein")
```
Analysis: The data seems fairly linear in nature however there are many outliers. As we can see that data is fairly distributed around the line drawn (above and below) thus there is little bias.

## 3.2 Multiple Models of varying degree.

$$ M_i = \sum_{i=0}^{p} X^{i}{Protein} * \beta{i} + \epsilon$$ 

$$\epsilon \sim N\left(0, \sigma^{2} \right)$$

$$\epsilon = M_i - \sum_{i=0}^{p} X^{i}{Protein} * \beta{i}$$

$$ M_i \sim N\left(\sum_{i=0}^{p} X^{i}{Protein} * \beta{i},   \sigma_{M}^{2}\right) $$

$$\text{or}$$

$$P \left(M_i | X_{Protein}, \vec{\beta} \right) = N\left(\sum_{i=0}^{p} X^{i}{Protein} * \beta{i},   \sigma_{M}^{2}\right)$$ 

$$Where,$$

$$\sigma_{M}^{2}: \text{variance of Moisture}$$

$$p: \text{degree of the polynomial}$$

## 3.3 Validation of the Model
```{r}

final_data <- tecator_data

magic_function <- function(df, N)
{
df2 <- df  
for(i in 2:N) 
{
  df2[paste("Protein_",i,"_power", sep="")] <- (df2$Protein)^i
  }

df2 <- df2[c("Protein_2_power", "Protein_3_power", 
             "Protein_4_power", "Protein_5_power", 
             "Protein_6_power")]

df <- cbind(df,df2)  
return(df)
}

final_data <- magic_function(final_data, 6)

set.seed(12345)
n =  NROW(final_data)
id = sample(1:n, floor(n*0.5))
train = final_data[id,]
test = final_data[-id,]

# model building
M_1 <- lm(data = train, Moisture~Protein)
M_2 <- lm(data = train, Moisture~Protein+Protein_2_power)
M_3 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power)
M_4 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power+
            Protein_4_power)
M_5 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power+
            Protein_4_power+Protein_5_power)
M_6 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power+
            Protein_4_power+Protein_5_power+Protein_6_power)

train$type <- "train"
test$type <- "test"

final_data <- rbind(test, train)

# predicting new values
M_1_predicted <- predict(M_1, newdata = final_data)
M_2_predicted <- predict(M_2, newdata = final_data)
M_3_predicted <- predict(M_3, newdata = final_data)
M_4_predicted <- predict(M_4, newdata = final_data)
M_5_predicted <- predict(M_5, newdata = final_data)
M_6_predicted <- predict(M_6, newdata = final_data)

# calculating the MSE
final_data$M_1_error <- (final_data$Moisture - M_1_predicted)^2
final_data$M_2_error <- (final_data$Moisture - M_2_predicted)^2
final_data$M_3_error <- (final_data$Moisture - M_3_predicted)^2
final_data$M_4_error <- (final_data$Moisture - M_4_predicted)^2
final_data$M_5_error <- (final_data$Moisture - M_5_predicted)^2
final_data$M_6_error <- (final_data$Moisture - M_6_predicted)^2

# Chainning like Chainsaw
final_error_data <- final_data %>% select(type, M_1_error, M_2_error, M_3_error, 
                                          M_4_error, M_5_error, M_6_error) %>% 
  gather(variable, value, -type) %>% 
  separate(variable, c("model", "power", "error"), "_") %>% 
  group_by(type, power) %>% 
  summarise(MSE = mean(value, na.rm=TRUE))

ggplot(final_error_data, aes(x = power, y = MSE, color=type)) + geom_point() + 
  ggtitle("Mean squared error vs. model complexitiy by dataset type")

```
Analysis: As evident from the plot above, we see that as we increase the model complexitiy (higher powers of the 'protein'), the trainning error reduces however the model becomes too biased towards the trainning set (overfits) and misses the test datasets prediction by larger margins in higher powers. 

The best model is M1, that is Moisture~Protein as evident from the least test error (MSE).

The above is a classical case of bias-varience trade-off, which is as follows, as one makes the model fit the trainning dataset better the model becomes more biased and its ability to handle variation to new dataset decreases(varience), thus one should also maintain a good trade off between these two.

## 3.4 Perform variable selection of a linear model in which Fat is response and Channel1:Channel100 are predicted by using stepAIC.
```{r warning=FALSE}
min.model1 = lm(Fat ~ 1, data=tecator_data[,-1])
biggest1 <- formula(lm(Fat ~.,  data=tecator_data[,-1]))

step.model1 <- stepAIC(min.model1, direction ='forward', scope=biggest1, trace = FALSE)
summ(step.model1)
```

Analysis: 29 variables were choose out of 107. Even among these there are many which have very low p values thus statistically it is a practice to remove variables which are above 0.0005 p values, thus the true variables may not even include these many.

## 3.5 Fit a Ridge regression model with the same predictor and response 

```{r}
y <- tecator_data %>% select(Fat) %>% data.matrix()
x <- tecator_data %>% select(Moisture, Protein, Channel100, Channel41,
Channel7, Channel48, Channel42, Channel50, Channel45,
Channel66, Channel56, Channel90, Channel60, Channel70,
Channel67, Channel59, Channel65, Channel58, Channel44,
Channel18, Channel78, Channel84, Channel62, Channel53,
Channel75, Channel57, Channel63, Channel24, Channel37) %>% data.matrix()

lambda <- 10^seq(10, -2, length = 100)

ridge_fit <- glmnet(x, y, alpha = 0, family = "gaussian", lambda = lambda)
plot(ridge_fit, xvar = "lambda", label = TRUE, 
     main = "Plot showing shrinkage of coefficents with rise in log of lambda")


## Chnage of coefficent with respect to lambda
result <- NULL
for(i in lambda){
temp <- t(coef(ridge_fit, i)) %>% as.matrix()
temp <- cbind(temp, lambda = i)
result <- rbind(temp, result)
}
result <- result %>% as.data.frame() %>% arrange(lambda)
```

```{r echo=TRUE, results='asis'}
table_cofe <- head(result, 10) %>% select(Channel18, Channel78, Channel84, Channel62, Channel53, Channel75, Channel57,Protein, lambda)

knitr::kable(table_cofe, caption = "Coefficent of Ridge Regression vs. Lambda")
```


## 3.6 Fit a Lasso regression model with the same predictor and response 

```{r}
lambda <- 10^seq(10, -2, length = 100)

lasso_fit <- glmnet(x, y, alpha = 1, family = "gaussian", lambda = lambda)
plot(lasso_fit, xvar = "lambda", label = TRUE, 
     main = "Plot showing shrinkage of coefficents with rise in log of lambda")

```
## 3.7 Use cross-validation to find the optimal LASSO model, report the optimal ðœ†ðœ† and how many variables were chosen by the model and make conclusions

```{r}
#find the best lambda from our list via cross-validation
lambda <- seq(from = 0, to = 100, length.out = 101)
lass_cv <- cv.glmnet(x,y, alpha=1, lambda = lambda, type.measure="mse")
best_lasso_lamda <- lasso_cv$lambda.min
plot(lass_cv)

`coef(lass_cv, lambda=lass_cv$lambda.1se)
``
# Apendix
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```
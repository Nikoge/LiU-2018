---
title: "machine learning(732A99) lab1"
author: "Anubhav Dikshit(anudi287)"
date: "26 November 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 1

## Loading The Libraries
```{r, message=FALSE, echo = FALSE}

if (!require("pacman")) install.packages("pacman")
pacman::p_load(xlsx, glmnet, MASS, jtools, huxtable, ggplot2, 
               ggthemes, gridExtra, ROCR, broom, caret, e1071,
               kknn, tidyr, dplyr,reshape2, glmnet)

options("jtools-digits" = 2, scipen = 999)

```

## Loading Input files
```{r}
spam_data <- read.xlsx("spambase.xlsx", sheetName = "spambase_data")
spam_data$Spam <- as.factor(spam_data$Spam)

tecator_data <- read.xlsx("tecator.xlsx", sheetName = "data")
```

## 1.1 Import the data into R and divide it into training and test sets (50%/50%) by using the following code
```{r}
set.seed(12345)

n =  NROW(spam_data)
id = sample(1:n, floor(n*0.5))
train = spam_data[id,]
test = spam_data[-id,]
```

## 1.2 Use logistic regression (functions glm(), predict()) to classify the training and test data by the classification principles
```{r, message=FALSE, warning=FALSE}

min.model = glm(Spam ~ 1, family=binomial, data=train)
biggest <- formula(glm(Spam ~., family=binomial, data=train))

step.model <- step(min.model, direction='forward', scope=biggest, trace = FALSE)
summary(step.model)
```

### Manual Feature Selection
```{r, warning=FALSE}
best_model <- glm(formula = Spam ~ Word35 + Word46 + Word42 + Word44 + Word33 + 
    Word45 + Word39 + Word48 + Word30 + Word43 + Word37 + 
    Word36 + Word31, family = binomial, data = train)
```

###  Prediction for probability greater than 50% and 90%
```{r}
# prediction
train$prediction_prob <- predict(best_model, newdata = train, type = "response")
test$prediction_prob <- predict(best_model, newdata = test , type = "response")

train$prediction_class_50 <- ifelse(train$prediction_prob > 0.50, 1, 0)
test$prediction_class_50 <- ifelse(test$prediction_prob > 0.50, 1, 0)

train$prediction_class_90 <- ifelse(train$prediction_prob > 0.90, 1, 0)
test$prediction_class_90 <- ifelse(test$prediction_prob > 0.90, 1, 0)
```

### Assessing the Model
```{r}
# plots
ggplot(train, aes(prediction_prob, color = Spam)) + 
geom_density(size = 1) + ggtitle("Training Set's Predicted Score for 50% cutoff") + 
  scale_color_economist(name = "data", labels = c("negative", "positive")) + 
  theme_economist()


ggplot(test, aes(prediction_prob, color = Spam)) + 
geom_density(size = 1) + ggtitle("Test Set's Predicted Score for 50% cutoff") + 
  scale_color_economist(name = "data", labels = c("negative", "positive")) +  
  theme_economist()

```

## 1.2 Assessing the Fit on train dataset for 50%
```{r}
#confusion table
conf_train <- table(train$Spam, train$prediction_class_50)
names(dimnames(conf_train)) <- c("Actual Train", "Predicted Train")
confusionMatrix(conf_train)

conf_test <- table(test$Spam, test$prediction_class_50)
names(dimnames(conf_test)) <- c("Actual Test", "Predicted Test")
confusionMatrix(conf_test)
```
Analysis: Distribution of the prediction score grouped by known outcome given that our model's final objective is to classify new instances into one of two categories (spam vs. non-spam). We will want the model to give high scores to positive instances (1: spam) and low scores (0 : not spam) otherwise.Ideally you want the distribution of scores to be separated, with the score of the negative instances to be on the left and the score of the positive instance to be on the right.

From the confusion matrix it is apparent that Accuracy on train and test dataset when cutoff=50% is about 83%, thus the misclassification rate is 17% on the train and test dataset.

## 1.3 Assessing the Fit on train dataset for 90%
```{r}

#confusion table
conf_train1 <- table(train$Spam, train$prediction_class_90)
names(dimnames(conf_train1)) <- c("Actual Train", "Predicted Train")
conf_train1

conf_test1 <- table(test$Spam, test$prediction_class_90)
names(dimnames(conf_test1)) <- c("Actual Test", "Predicted Test")
conf_test1

```

Analysis: Strange, the model only predicts one class!! We know that the prediction of a logistic regression model is a probability, thus in order to use it as a classifier, we'll have to choose a cutoff value, or threshold (cutoff). Where scores above this value will classified as positive, those below as negative. Lets us find this optimum value.

### Choosing the best cutoff for test
```{r}

cutoffs <- seq(from = 0.05, to = 0.95, by = 0.05)
accuracy <- NULL

for (i in seq_along(cutoffs)){
    prediction <- ifelse(test$prediction_prob >= cutoffs[i], 1, 0) #Predicting for cut-off

    accuracy <- c(accuracy,length(which(test$Spam == prediction))/length(prediction)*100)}

cutoff_data <- as.data.frame(cbind(cutoffs, accuracy))

ggplot(data = cutoff_data, aes(x = cutoffs, y = accuracy)) + 
  geom_line() + 
  ggtitle("Cutoff vs. Accuracy for Test Dataset")

```

Analysis: Our small detour suggests that the cutoff value of 50% was the best for our purpose and going higher than this leads to worse results, at 0.8 and above the accuracy drastically reduces which is what we see when we make cutoff as 0.9.

From the confusion matrix it is evident that the model becomes a trivial model(predicts all cases as one class) and thus the prediction is no better than tossing a coin. This should be the absoutely the worst case that we should avoid.

The missclassication rate is about 31% for both the trainning dataset and test dataset.

## 1.4 Use standard classifier kknn() with K=30 from package kknn, report the the misclassification rates for the training and test data and compare the results with step 1.2.

```{r}
knn_model30 <- train.kknn(Spam ~ Word35 + Word46 + Word42 + Word44 + Word33 + 
    Word45 + Word39 + Word48 + Word30 + Word43 + Word37 + 
    Word36 + Word31, data = train, kmax = 30)

train$knn_prediction_class <- predict(knn_model30, train)
test$knn_prediction_class <- predict(knn_model30, test)

conf_train2 <- table(train$Spam, train$knn_prediction_class)
names(dimnames(conf_train2)) <- c("Actual Train", "Predicted Train")
confusionMatrix(conf_train2)

conf_test2 <- table(test$Spam, test$knn_prediction_class)
names(dimnames(conf_test2)) <- c("Actual Test", "Predicted Test")
confusionMatrix(conf_test2)
```

Analysis: Using KKNN with K = 30, we increased our trainning accuracy to 90%, thus misclassification is 10%, however for the test dataset misclassification rate is about 15%.

Thus compared to using logisitc model the misclassification error for the trainning dataset decreased by 7% to just 10%, while for the test dataset the misclassification error decreased only by 2% to 15%. 

## 1.5 Repeat step 4 for K=1 and compare the results with step 4. What effect does the decrease of K lead to and why?
```{r}
knn_model1 <- train.kknn(Spam ~ Word35 + Word46 + Word42 + Word44 + Word33 + 
    Word45 + Word39 + Word48 + Word30 + Word43 + Word37 + 
    Word36 + Word31, data = train, kmax = 1)

train$knn_prediction_class <- predict(knn_model1, train)
test$knn_prediction_class <- predict(knn_model1, test)

conf_train2 <- table(train$Spam, train$knn_prediction_class)
names(dimnames(conf_train2)) <- c("Actual Train", "Predicted Train")
confusionMatrix(conf_train2)

conf_test2 <- table(test$Spam, test$knn_prediction_class)
names(dimnames(conf_test2)) <- c("Actual Test", "Predicted Test")
confusionMatrix(conf_test2)
```

Analysis: Using KKNN with K = 1, we increased our trainning accuracy to 96%, thus misclassification is 4%, however for the test dataset accuracy is 83% thus misclassification rate is about 17%, thus we improved on the trainning accuracy but did bad on the test case, thus this is an example of overfitting more bias.

Explaination: The KKNN works in the following way,  An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Thus K=1, makes the seperation boundary to be very complex and locally optimised (lots of local clusters), while as K goes higher, the decision boundary becomes more linear/simple.

# Assignment 2 Feature selection by cross-validation in a linear model

## 2.1 Implement an R function that performs feature selection (best subset selection) in linear regression by using k-fold cross-validation without using any specialized function like lm() (use only basic R functions)

```{r}

subset_function <- function(X,Y,N){

X = swiss[,1:5]
Y = swiss[,6:6]
N = 5

df <- cbind(X,Y)
temp <- NULL
 
for(i in 1:NCOL(X)){
combs <- as.data.frame(gtools::combinations(NCOL(X), r=i, v=colnames(X), repeats.allowed=FALSE))
combs <- tidyr::unite(combs, "formula", sep = ",")
temp <- rbind(combs, temp)
}  
  
set.seed(12345)
df2 <- df[sample(nrow(df)),]
df2$k_fold <- sample(N, size = nrow(df), replace = TRUE)
 
result <- NULL 

for (j in 1:NROW(temp))
{
  for(i in 1:N){

train = df2[df2$k_fold != i,]
test = df2[df2$k_fold == i,]
y_train = train[,c("Y")] 

vec <- temp[j,]
train = lapply(strsplit(as.character(vec), ","), function(x) train[x])[[1]]
test = lapply(strsplit(as.character(vec), ","), function(x) test[x])[[1]]

train = as.matrix(train)
test = as.matrix(test)
y_train = as.matrix(y_train) 


t_train =  t(train) 
t_test =  t(test) 

betas = solve(t_train %*% train) %*% t_test %*% y_train
y_hat_val = X_val %*% betas
mse = mean((y_val - y_hat_val) ^ 2)
     
}




model <- lm(formula = model_forumla, data = train)
predicted <- predict(model, newdata = test)

RMSE <- sqrt(mean((predicted - test$Y)^2))
data <- cbind(i, temp[j,], RMSE)
result <- rbind(data, result)

}
}

result <- as.data.frame(result)

colnames(result) <- c("kfold", "variables", "rmse")

result$rmse <- as.numeric(result$rmse)
result$no_variables <- nchar(as.character(result$variables)) 
- nchar(gsub('\\+', "", result$variables)) + 1

variable_performance <- result %>% 
  group_by(kfold, no_variables) %>% 
  summarise(RMSE = mean(rmse, na.rm = TRUE))

myplot <- ggplot(data = variable_performance, aes(x = no_variables, y = RMSE, color=kfold)) + 
geom_line() + ggtitle("Plot of RMSE vs. Number of variables by folds")

myplot2 <- ggplot(data = result, aes(x = variables, y = rmse, color=kfold)) + 
geom_bar(stat="identity") + ggtitle("Plot of RMSE vs. Features by folds") + coord_flip() 

return(list(myplot, myplot2))
}
```


## 2.2 Test your function on data set swiss available in the standard R repository:
```{r}
#subset_function(X = swiss[,1:5], Y = swiss[,6], N = 5)
```
Analysis:

# Assignment 3 Linear regression and regularization

## 3.1 Import data to R and create a plot of Moisture versus Protein. Do you think that these data are described well by linear model.
```{r}
ggplot(data = tecator_data, aes(x = Protein, y = Moisture)) + 
  geom_point() + 
  geom_smooth() +
  ggtitle("Plot of Moisture vs. Protein")
```
Analysis: The data seems fairly linear in nature however there are many outliers. As we can see that data is fairly distributed around the line drawn (above and below) thus there is little bias.

## 3.2 Multiple Models of varying degree.

$$ M_i = \sum_{i=0}^{p} X^{i}{Protein} * \beta{i} + \epsilon$$ 

$$\epsilon \sim N\left(0, \sigma^{2} \right)$$

$$\epsilon = M_i - \sum_{i=0}^{p} X^{i}{Protein} * \beta{i}$$

$$ M_i \sim N\left(\sum_{i=0}^{p} X^{i}{Protein} * \beta{i},   \sigma_{M}^{2}\right) $$

$$\text{or}$$

$$P \left(M_i | X_{Protein}, \vec{\beta} \right) = N\left(\sum_{i=0}^{p} X^{i}{Protein} * \beta{i},   \sigma_{M}^{2}\right)$$ 

$$Where,$$

$$\sigma_{M}^{2}: \text{variance of Moisture}$$

$$p: \text{degree of the polynomial}$$

Analysis: Thus the model that fits the Mositure is normally distributed with mean of 63.2044 and standard distribution of 9.87

## 3.3 Validation of the Model
```{r}

final_data <- tecator_data

magic_function <- function(df, N)
{
df2 <- df  
for(i in 2:N) 
{
  df2[paste("Protein_",i,"_power", sep="")] <- (df2$Protein)^i
  }

df2 <- df2[c("Protein_2_power", "Protein_3_power", 
             "Protein_4_power", "Protein_5_power", 
             "Protein_6_power")]

df <- cbind(df,df2)  
return(df)
}

final_data <- magic_function(final_data, 6)

set.seed(12345)
n =  NROW(final_data)
id = sample(1:n, floor(n*0.5))
train = final_data[id,]
test = final_data[-id,]

# model building
M_1 <- lm(data = train, Moisture~Protein)
M_2 <- lm(data = train, Moisture~Protein+Protein_2_power)
M_3 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power)
M_4 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power+
            Protein_4_power)
M_5 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power+
            Protein_4_power+Protein_5_power)
M_6 <- lm(data = train, Moisture~Protein+Protein_2_power+Protein_3_power+
            Protein_4_power+Protein_5_power+Protein_6_power)

train$type <- "train"
test$type <- "test"

final_data <- rbind(test, train)

# predicting new values
M_1_predicted <- predict(M_1, newdata = final_data)
M_2_predicted <- predict(M_2, newdata = final_data)
M_3_predicted <- predict(M_3, newdata = final_data)
M_4_predicted <- predict(M_4, newdata = final_data)
M_5_predicted <- predict(M_5, newdata = final_data)
M_6_predicted <- predict(M_6, newdata = final_data)

# calculating the MSE
final_data$M_1_error <- (final_data$Moisture - M_1_predicted)^2
final_data$M_2_error <- (final_data$Moisture - M_2_predicted)^2
final_data$M_3_error <- (final_data$Moisture - M_3_predicted)^2
final_data$M_4_error <- (final_data$Moisture - M_4_predicted)^2
final_data$M_5_error <- (final_data$Moisture - M_5_predicted)^2
final_data$M_6_error <- (final_data$Moisture - M_6_predicted)^2

# Chainning like Chainsaw
final_error_data <- final_data %>% select(type, M_1_error, M_2_error, M_3_error, 
                                          M_4_error, M_5_error, M_6_error) %>% 
  gather(variable, value, -type) %>% 
  separate(variable, c("model", "power", "error"), "_") %>% 
  group_by(type, power) %>% 
  summarise(MSE = mean(value, na.rm=TRUE))

ggplot(final_error_data, aes(x = power, y = MSE, color=type)) + geom_point() + 
  ggtitle("Mean squared error vs. model complexitiy by dataset type")

```
Analysis: As evident from the plot above, we see that as we increase the model complexitiy (higher powers of the 'protein'), the trainning error reduces however the model becomes too biased towards the trainning set (overfits) and misses the test datasets prediction by larger margins in higher powers. 

The best model is M1, that is Moisture~Protein as evident from the least test error (MSE).

The above is a classical case of bias-varience trade-off, which is as follows, as one makes the model fit the trainning dataset better the model becomes more biased and its ability to handle variation to new dataset decreases(varience), thus one should also maintain a good trade off between these two.

## 3.4 Perform variable selection of a linear model in which Fat is response and Channel1:Channel100 are predicted by using stepAIC.
```{r, results='hide', message=FALSE, warning=FALSE, echo=TRUE, include=TRUE}
min.model1 = lm(Fat ~ 1, data=tecator_data[,-1])
biggest1 <- formula(lm(Fat ~.,  data=tecator_data[,-1]))

step.model1 <- stepAIC(min.model1, direction ='forward', scope=biggest1, trace = FALSE)
```

```{r}
summary(step.model1)

```
Analysis: 29 variables were choose out of 107. Even among these there are many which have very low p values thus statistically it is a practice to remove variables which are above 0.0005 p values, thus the true variables may not even include these many.

## 3.5 Fit a Ridge regression model with the same predictor and response 

```{r}
y <- tecator_data$Fat
x <- tecator_data %>% data.matrix()
lambdas <- 10^seq(3, -2, by = -.1)

ridge_fit <- cv.glmnet(x, y, alpha = 0, lambda = lambdas)
dim(coef(ridge_fit))
plot(ridge_fit)


opt_lambda <- ridge_fit$lambda.min
best_ridge <- ridge_fit$glmnet.fit

```


# Apendix
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```
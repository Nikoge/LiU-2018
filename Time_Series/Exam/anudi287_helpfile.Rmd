---
title: "Time Series Analysis Helpfile"
author: "Anubhav Dikshit (anudi287)"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
header-includes:
  - \usepackage{booktabs} 
  - \usepackage{sectsty} \sectionfont{\centering}
  - \renewcommand{\contentsname}{}\vspace{-2cm} 
  - \usepackage{pdfpages}
---

\newpage

# Library
```{r setup, include=TRUE, message=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library("tidyverse") #ggplot and dplyr 
library("gridExtra") # combine plots
library("knitr") # for pdf
library("fpp2") #timeseries with autoplot and stuff
library("reshape2") #reshape the data
library("MASS") #StepAIC
library("astsa") #dataset oil and gas is present here
library("zoo") #dataset oil and gas is present here
library("forecast") # for forecasting time series
library("kernlab") #gausspr function
library("TSA") #Q3
library("tseries")
library("matlib") # for inv and I

# The palette with black:
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

set.seed(12345)
options(scipen=999)

```

\newpage

## Generate two time series and use smoothing filter

\fbox{\begin{minipage}{46.7em}
a) Generate two time series $x_t = -0.8x_{t-2} + w_t,$ where $x_0 = x_1 = 0$ and $x_t= cos(\frac{2 \pi t}{5}) $ with 100 observations each. Apply a smoothing filter $v_t = 0.2(x_t + x_{t-1} + x_{t-2} + x_{t-3} + x_{t-4})$ to these two series and compare how the filter has affected them.
\end{minipage}}

```{r}
set.seed(12345)

n = 100
x <- vector(length = n)
x2 <- vector(length = n)

x[1] <- 0
x[2] <- 0

#first series generation
for(i in 3:n){
  x[i] <- -0.8 * x[i-2] + rnorm(1,0,1)
}

#second series generation
for(i in 1:n){
  x2[i] <- cos(0.4*pi*i)
}

# smoothing filter function
smoothing_filter <- function(x){
v <- vector(length = length(x))
for(i in 5:length(x)){
  v[i] = 0.2 * (x[i] + x[i-1] + x[i-2] + x[i-3] + x[i-4])
}
return(v)
}

#generate smoothed series
smooth_x <- smoothing_filter(x)
smooth_x2 <- smoothing_filter(x2)

#adding everything to a dataframe
df <- cbind(x,x2,smooth_x,smooth_x2) %>% as.data.frame() %>% mutate(index=1:100)

ggplot(df, aes(x=index)) + 
  geom_line(aes(y=x, color="Original Time Series")) + 
  geom_line(aes(y=smooth_x, color="Smoothed Time Series")) + 
  ggtitle("Plot of 1st time series and its smoothed version") +
    scale_colour_manual("", breaks = c("Original Time Series", "Smoothed Time Series"),
                        values = c("#CC79A7", "#000000"))

ggplot(df, aes(x=index)) + 
  geom_line(aes(y=x2, color="Original Time Series")) + 
  geom_line(aes(y=smooth_x2, color="Smoothed Time Series")) + 
  ggtitle("Plot of 2ND time series and its smoothed version") +
    scale_colour_manual("", breaks = c("Original Time Series", "Smoothed Time Series"),
                        values = c("#CC79A7", "#000000"))
```

## Casuality and Invertiblity.

\fbox{\begin{minipage}{46.7em}
b) Consider time series $x_t - 4 x_{t-1} + 2 x_{t-2} + x_{t-5} = w_t + 3 w_{t-2} + w_{t-4} - 4w_{t-6}$. Write an appropriate R code to investigate whether this time series is casual and invertible.
\end{minipage}}

Causality:
ARMA(p,q) is causal iff roots $\phi(z')=0$ are outside unit circle. 
eg: $x_t = 0.4x_{t-1} + 0.3x_{t-2} + w_t$, roots are -> $1-0.4B+0.3B^2$

equation is: $\phi(Z) = 1-4B+2B^2+0 B^3 +0 B^4 + B^5$
```{r}
z = c(1,-4,2,0,0,1)
polyroot(z)
any(Mod(polyroot(z))<=1)
```

Invertible:
ARMA(p,q) is causal iff roots $\theta(z')=0$ are outside unit circle. 

equation is: $\theta(Z) = 1+3B^2+B^4-4B^6$
```{r}
z = c(1,0,3,0,1,0,-4)
polyroot(z)
any(Mod(polyroot(z))<=1)
```

## ACF and Theortical ACF

\fbox{\begin{minipage}{46.7em}
c) Use built-in R functions to simulate 100 observations from the process $ x_t + \frac{3}{4} x_{t-1} = w_t - \frac{1}{9} w_{t-2}$ compute sample ACF and theoretical ACF, use seed 54321. Compare the ACF plots.
\end{minipage}}

```{r}
set.seed(54321)

series <- arima.sim(n = 100, list(ar = c(-3/4), ma = c(0,-1/9)))

acf(series)
acf(ARMAacf(ar = c(-3/4), ma =  c(0,-1/9), lag.max = 20))
```


# Visualization, detrending and residual analysis of Rhine data.
The data set Rhine.csv contains monthly concentrations of total nitrogen in the Rhine River in the period 1989-2002.

## ACF Plot

\fbox{\begin{minipage}{46.7em}
a) Import the data to R, convert it appropriately to ts object (use function ts()) and explore it by plotting the time series, creating scatter plots of $x_t$ against $x_{t-1},....x_{t-12}$. Analyze the time series plot and the scatter plots: Are there any trends, linear or seasonal, in the time series? When during the year is the concentration highest? Are there any special patterns in the data or scatter plots? Does the variance seem to change over time? Which variables in the scatter plots seem to have a significant relation to each other?
\end{minipage}}


```{r}
set.seed(12345)
rhine_data <- read.csv2("Rhine.csv")
rhine_data_ts <- ts(data = rhine_data$TotN_conc, 
                    start = c(1989,1), 
                    frequency = 12)

plot.ts(rhine_data_ts, main="Time Series of Nitrogen Concentration in Rhine")
lag.plot(rhine_data_ts,lags = 12)  
acf(rhine_data_ts)

#alternative
autoplot(rhine_data_ts) + ylab("Total Concentration") +xlab("Year") +
  ggtitle("Concentration of Nitrogen in Rhine vs. Year")

gglagplot(rhine_data_ts, lags = 1, set.lags = 1:12, color=FALSE) 
ggAcf(rhine_data_ts) + ggtitle("ACF for Total Nitrogen Concentration")

```

## Detrending using linear regression

\fbox{\begin{minipage}{46.7em}
b) Eliminate the trend by fitting a linear model with respect to t to the time series. Is there a significant time trend? Look at the residual pattern and the sample ACF of the residuals and comment how this pattern might be related to seasonality of the series.
\end{minipage}}

```{r}
set.seed(12345)

rhine_lm_model <- lm(TotN_conc~Time, data=rhine_data)
plot(rhine_lm_model$residuals, type = 'l', main="Plot of Residual from the 
     linear model of Nitrogen Concentration")
acf(rhine_lm_model$residuals)

```

## Detrending using kernel smoother

\fbox{\begin{minipage}{46.7em}
c) Eliminate the trend by fitting a kernel smoother with respect to t to the time series (choose a reasonable bandwidth yourself so the fit looks reasonable). Analyze the residual pattern and the sample ACF of the residuals and compare it to the ACF from step b). Conclusions? Do residuals seem to represent a stationary series?
\end{minipage}}

```{r}
set.seed(12345)

model_smooth_lag_5 <- ksmooth(x = rhine_data$Time, y = rhine_data$TotN_conc, 
                              bandwidth=5)
model_smooth_lag_10 <- ksmooth(x = rhine_data$Time, y = rhine_data$TotN_conc, 
                               bandwidth=10)
model_smooth_lag_20 <- ksmooth(x = rhine_data$Time, y = rhine_data$TotN_conc, 
                               bandwidth=20)

model_smooth_lag_5_residual <- rhine_data$TotN_conc - model_smooth_lag_5$y
model_smooth_lag_10_residual <- rhine_data$TotN_conc - model_smooth_lag_10$y
model_smooth_lag_20_residual <- rhine_data$TotN_conc - model_smooth_lag_20$y

residual_df <- cbind(model_smooth_lag_5_residual, model_smooth_lag_10_residual, 
                     model_smooth_lag_20_residual, rhine_data$Time) %>% 
  as.data.frame()

colnames(residual_df) <- c("lag_5_residual", "lag_10_residual", 
                           "lag_20_residual", "Time")

ggplot(residual_df, aes(x=Time)) + 
  geom_line(aes(y=lag_5_residual, color="Lag 5 residual")) + 
  geom_line(aes(y=lag_10_residual, color="Lag 10 residual")) + 
  geom_line(aes(y=lag_20_residual, color="Lag 20 residual")) + 
  ggtitle("Residual vs. Time by Lag") +
    scale_colour_manual("", breaks = c("Lag 5 residual", "Lag 10 residual", 
                                       "Lag 20 residual"),
                        values = c("#CC79A7", "#000000", "#D55E00"))


acf(model_smooth_lag_5_residual)
acf(model_smooth_lag_10_residual)
acf(model_smooth_lag_20_residual)
```

## Detrending using seasonal means model

\fbox{\begin{minipage}{46.7em}
d) Eliminate the trend by fitting the following so-called seasonal means model:
$x_t = \alpha_0 + \alpha_1 t + \beta_1 I(month=2)+.....+ \beta_{12} I(month = 12) + w_t$, where I(x)=1 is an identity function. Fitting of this model will require you to augment data with a categorical variable showing the current month, and then fitting a usual linear regression. Analyze the residual pattern and the ACF of residuals.
\end{minipage}}

```{r}
set.seed(12345)

rhine_data_wide <- rhine_data
rhine_data_wide$dummy <- "1"
rhine_data_wide$Month <- paste0("Month_",rhine_data_wide$Month)
rhine_data_wide <- dcast(rhine_data_wide, 
                         formula = TotN_conc+Year+Time~Month, 
                         value.var = "dummy", fill = "0")

lm_model_month_lag <- lm(data=rhine_data_wide, 
                    TotN_conc~Time+Month_1+Month_2+Month_3+Month_4+Month_5+
                      Month_6+
                      Month_7+
                      Month_8+Month_9+Month_10+Month_11+Month_12)


plot(lm_model_month_lag$residuals, type = 'l', main="Plot of the Residuals vs. Time")
acf(lm_model_month_lag$residuals)

```

## Model tuning using SetpAIC

\fbox{\begin{minipage}{46.7em}
e) Perform step-wise variable selection in model from step d). Which model gives you the lowest AIC value? Which variables are left in the model?
\end{minipage}}

```{r}
set.seed(12345)

lm_model_month_lag_step <- stepAIC(lm_model_month_lag, 
                                   scope = list(upper = ~Time+Month_1+Month_2+
                                                        Month_3+Month_4+Month_5+
                                                        Month_6+Month_7+
                                                        Month_8+Month_9+Month_10+
                                                        Month_11+Month_12, 
                                                                  lower = ~1), 
                                   trace = TRUE, 
                                   direction="backward")

colnames(lm_model_month_lag_step$model)
```

# Analysis of oil and gas time series


Weekly time series oil and gas present in the package astsa show the oil prices in dollars per barrel and gas prices in cents per dollar.

## Checking Stationary

\fbox{\begin{minipage}{46.7em}
a) Plot the given time series in the same graph. Do they look like stationary series? Do the processes seem to be related to each other? Motivate your answer.
\end{minipage}}


```{r}
set.seed(12345)

data_oil <- astsa::oil
data_gas <- astsa::gas

ts.plot(data_oil, data_gas, gpars = list(col = c("black", "red")))

#alternative

autoplot(ts(cbind(data_oil, data_gas), start = 2000, frequency = 52)) + 
           ylab("Price of Oil and Gas") +xlab("Year") + 
           ggtitle("Price of Oil and Gas vs. Years")
```


## Log transformation to fix stationary

\fbox{\begin{minipage}{46.7em}
b) Apply log-transform to the time series and plot the transformed data. In what respect did this transformation made the data easier for the analysis?
\end{minipage}}

```{r}
set.seed(12345)

autoplot(ts(cbind(log(data_oil), log(data_gas)), start = 2000, frequency = 52)) + 
           ylab("Log of price of Oil and Gas") +xlab("Year") + 
           ggtitle("Log of price of Oil and Gas vs. Years")
```

## Detrending using difference method

\fbox{\begin{minipage}{46.7em}
c) To eliminate trend, compute the first difference of the transformed data, plot the detrended series, check their ACFs and analyze the obtained plots. Denote the data obtained here as $x_t$(oil) and $y_t$(gas).
\end{minipage}}

```{r}
set.seed(12345)

autoplot(ts(diff(log(data_oil), differences = 1), start = 2000, frequency = 52)) + 
           ylab("Price of Oil") +xlab("Year") + 
           ggtitle("Price of Oil with Diff 1 vs. Years")

autoplot(ts(diff(log(data_gas), differences = 1), start = 2000, frequency = 52)) + 
           ylab("Price of Gas") +xlab("Year") + 
           ggtitle("Price of Gas with Diff 1 vs. Years")

ggAcf(diff(log(data_oil), differences = 1), data_oil)
ggAcf(diff(log(data_gas), differences = 1), data_gas)

```

## Detrending using smoother

\fbox{\begin{minipage}{46.7em}
d) Exhibit scatter plots of $x_t$ and $y_t$ for up to three weeks of lead time of $x_t$ include a non-parametric smoother in each plot and comment the results: are there outliers? Are the relationships linear? Are there changes in the trend?
\end{minipage}}

```{r}
set.seed(12345)

oil_price_one_diff <- diff(log(data_oil), differences = 1) 
gas_price_one_diff <- diff(log(data_gas), differences = 1) 

df <- data.frame(oil_price_one_diff=as.matrix(oil_price_one_diff), 
           gas_price_one_diff = as.matrix(gas_price_one_diff), 
                      time=time(oil_price_one_diff))

df <- na.omit(df)

df$gas_price_one_diff = lag(df$gas_price_one_diff,1)
df$gas_price_two_diff = lag(df$gas_price_one_diff,2)
df$gas_price_three_diff = lag(df$gas_price_one_diff,3)


df <- na.omit(df)

df$smooth_one_week_lag <- ksmooth(x = df$oil_price_one_diff, 
                                  y = df$gas_price_one_diff, 
                                  bandwidth = 0.05, kernel = "normal")$y
df$smooth_two_week_lag <- ksmooth(x = df$oil_price_one_diff, 
                                  y = df$gas_price_two_diff, 
                                  bandwidth = 0.05, kernel = "normal")$y
df$smooth_three_week_lag <- ksmooth(x = df$oil_price_one_diff, 
                                    y = df$gas_price_three_diff, 
                                    bandwidth = 0.05, kernel = "normal")$y

df <- na.omit(df)

ggplot(data=df, aes(x=oil_price_one_diff, y = gas_price_one_diff)) + geom_point() +
  geom_line(aes(y= smooth_one_week_lag, color= "smooth_one_week_lag")) +
      scale_colour_manual("", breaks = c("smooth_one_week_lag"),
                        values = c("#CC79A7")) +
  ggtitle("Smoothed Plot of one week lag")

ggplot(data=df, aes(x=oil_price_one_diff, y = gas_price_two_diff)) + geom_point() +
    geom_line(aes(y= smooth_two_week_lag, color= "smooth_two_week_lag")) +
      scale_colour_manual("", breaks = c("smooth_two_week_lag"),
                        values = c("#56B4E9")) +
  ggtitle("Smoothed Plot of two week lag")


ggplot(data=df, aes(x=oil_price_one_diff, y = gas_price_three_diff)) + geom_point() +
    geom_line(aes(y= smooth_three_week_lag, color= "smooth_three_week_lag")) +
      scale_colour_manual("", breaks = c("smooth_three_week_lag"),
                        values = c("#D55E00")) +
  ggtitle("Smoothed Plot of three week lag")

```


## Detrending using linear regression

\fbox{\begin{minipage}{46.7em}
e) Fit the following model: $y_t = \alpha_0 + \alpha_1 I(x_t > 0) + \beta_1 x_t + \beta_2 x_{t-1} + w_t$ and check which coefficients seem to be significant. How can this be interpreted? Analyze the residual pattern and the ACF of the residuals.
\end{minipage}}

```{r}
set.seed(12345)

df$oil_price_two_diff = lag(df$oil_price_one_diff,2)
df$x_t_more_zero <- ifelse(df$oil_price_one_diff>0,"1","0")
lm_model_lag <- lm(data=df, formula = gas_price_one_diff~x_t_more_zero+
                     oil_price_one_diff+oil_price_two_diff)
summary(lm_model_lag)

plot(lm_model_lag$residuals, type = 'l', main="Residual vs. Time")
ggAcf(lm_model_lag$residuals)
```


# Linear Regressions on Necessarily Lagged Variables and Appropriate Correlation

## Generate AR and using PACF

\fbox{\begin{minipage}{46.7em}
a) Generate $1000$ observations from AR(3) process with $\phi_1 = 0.8, \phi_2 = -0.2, \phi_3 = 0.1$. Use these data and the definition of PACF to compute $\phi_{33}$ from the sample, i.e. write your own code that performs linear regressions on necessarily lagged variables and then computes an appropriate correlation. Compare the result with the output of function `pacf()` and with the theoretical value of $\phi_{33}$.
\end{minipage}}

$\phi_{33} = corr(X_{t-3}-f_p,X_t-f_p)$ where $f_p=\sum_{j=1}^p \phi_j X_{\tau-j}$

```{r}

set.seed(12345)
x_t <- arima.sim(model = list(ar = c(0.8,-0.2,0.1)), n=1000)
actual_pacf_value <- pacf(x_t, plot = FALSE)$acf[3]
df <- data.frame(x_t = as.vector(x_t))
df$x_t_lag_1 <- lag(df$x_t,1)
df$x_t_lag_2 <- lag(df$x_t,2)
df$x_t_lag_3 <- lag(df$x_t,3)
df <- na.omit(df)

# building models and getting their residuals
model_1_res <- lm(x_t ~ x_t_lag_1 + x_t_lag_2, data = df)$residuals
model_2_res <- lm(x_t_lag_3 ~ x_t_lag_1 + x_t_lag_2, data = df)$residuals

# theortical pacf values
theotical_pacf_value <- cor(x = model_1_res, y = model_2_res, 
                            use = "na.or.complete")

cat("The theoretical and actual value of PACF are: ", theotical_pacf_value, 
    actual_pacf_value)

```


## Methods of Moments, Conditional Least Squares and Maximum Likelihood

\fbox{\begin{minipage}{46.7em}
b) Simulate an AR(2) series with $\phi_1 = 0.8, \phi_2 = 0.1$ and $n=100$. Compute the estimated parameters and their standard errors by using three methods: method of moments (Yule-Walker equations), conditional least squares and maximum likelihood (ML) and compare their results to the true values. Which method does seem to give the best result? Does theoretical value for $\phi_2$ fall within confidence interval for ML estimate?
\end{minipage}}


```{r, warning=FALSE}
set.seed(12345)
x_t <- arima.sim(model = list(ar = c(0.8,0.1)), n=100)

method_yule_walker <- ar(x_t, order = 2, method = "yule-walker", aic = FALSE)$ar
method_cls <- ar(x_t, order = 2, method = "ols", aic = FALSE)$ar
method_mle <- ar(x_t, order = 2, method = "mle", aic = FALSE)$ar

df <- data.frame(rbind(method_yule_walker, method_cls,method_mle))

kable(df, caption = "Comparison of parameters using different methods")

# Since varience is not given by ar we use arima function
ML_Model_CI = arima(x_t, order = c(2,0,0), method = "ML")
sigma = ML_Model_CI$var.coef[2, 2]
phi_2 = ML_Model_CI$coef[2]
CI = c(phi_2 - 1.96 * sigma, phi_2 + 1.96 * sigma)
CI
```

# ARIMA

## Sample and Theoretical ACF and PACF

\fbox{\begin{minipage}{46.7em}
c) Generate $200$ observations of a seasonal $\text{ARIMA}(0,0,1) \times (0,0,1)_{12}$ model with coefficients $\Theta = 0.6$ and $\theta = 0.3$ by using `arima.sim()`. Plot sample ACF and PACF and also theoretical ACF and PACF. Which patterns can you see at the theoretical ACF and PACF? Are they repeated at the sample ACF and PACF?
\end{minipage}}

Now $ARIMA(1,1,1)(1,1,1)_4$ can be written as $(1-\phi_1B)(1-B)(1-B^4)(1-\Phi_1 B^4)x_t = w_t (1+\theta B)(1+\Theta B^4)$ Similarly $ARIMA(0,0,1)(0,0,1)_{12}$ can be written as $x_t=w_t(1+\Theta B^{12})(1+\theta B)$ which can be simplified as $x_t = w_t(1+\Theta B^{12}+ \theta B + \Theta \theta B^{13})$ given that $\theta=0.3$ and $\Theta =0.6$ we get $x_t =w_t(1+0.3B+0.6B^{12}+0.18B^{13})$

```{r}
set.seed(12345)
x_t <- arima.sim(model = list(ma = c(0.3,rep(0,10),0.6,0.18)), n=200)

df <- data.frame(sample_acf = acf(x_t, plot = FALSE, lag.max = 14)$acf,
                 sample_pacf = pacf(x_t, plot = FALSE, lag.max = 14)$acf,
                 theortical_acf = ARMAacf(ma = c(0.3,rep(0,10),0.6,0.18), 
                                          pacf = FALSE, lag.max = 13),
                 theortical_pacf = ARMAacf(ma = c(0.3,rep(0,10),0.6,0.18), 
                                           pacf = TRUE, lag.max = 14))

df$index <- rownames(df)

plot1 <- ggplot(data=df, aes(x=index)) + 
  geom_col(aes(y=sample_acf)) + 
  ggtitle("Sample ACF")

plot2 <- ggplot(data=df, aes(x=index)) + 
  geom_col(aes(y=theortical_acf)) + 
  ggtitle("Theoretical ACF")

grid.arrange(plot1, plot2, ncol = 1)

plot3 <- ggplot(data=df, aes(x=index)) + 
  geom_col(aes(y=sample_pacf)) + 
  ggtitle("Sample PACF")

plot4 <- ggplot(data=df, aes(x=index)) + 
  geom_col(aes(y=theortical_pacf)) + 
  ggtitle("Theoretical PACF")

grid.arrange(plot3, plot4, ncol = 1)

```


## Forecast and Prediction

\fbox{\begin{minipage}{46.7em}
d) Generate $200$ observations of a seasonal $\text{ARIMA}(0,0,1) \times (0,0,1)_{12}$ model with coefficients $\Theta = 0.6$ and $\theta = 0.3$ by using `arima.sim()`. Fit $\text{ARIMA}(0,0,1) \times (0,0,1)_{12}$ model to the data, compute forecasts and a prediction band $30$ points ahead and plot the original data and the forecast with the prediction band. Fit the same data with function `gausspr()` from package `kernlab` (use default settings). Plot the original data and predicted data from $t = 1$ to $t = 230$. Compare the two plots and make conclusions.
\end{minipage}}

```{r, message=FALSE}
set.seed(12345)
x_t <- arima.sim(model = list(ma = c(0.3,rep(0,10),0.6,0.18)), n=200)
fit_x_t <- arima(x_t, order = c(0,0,1), seasonal = list(order = c(0,0,1),
                                                        period = 12))
predicted_x_t <- predict(fit_x_t, n.ahead=30) 
predicted_x_t_upper_band <- predicted_x_t$pred + 1.96 * predicted_x_t$se
predicted_x_t_lower_band <- predicted_x_t$pred - 1.96 * predicted_x_t$se

#kernlab

df <- data.frame(y = x_t)
df$x <- as.numeric(rownames(df))
gausspr_model <- gausspr(x=df$x, y=df$y)
predicted_x_t_kernlab <- predict(gausspr_model, newdata=data.frame(x=201:230))

df3 <- data.frame(y = predicted_x_t_kernlab, x=201:230) 


df2 <- data.frame(predicted_x_t = predicted_x_t$pred, 
                  predicted_x_t_upper = predicted_x_t_upper_band, 
                  predicted_x_t_lower = predicted_x_t_lower_band,
                  x = 201:230)

ggplot() + 
  geom_line(data=df, aes(x=x, y=y, color="Actual y")) + 
    geom_line(data=df2, aes(x=x, y=predicted_x_t, color="Predicted y")) + 
      geom_line(data=df2, aes(x=x, y=predicted_x_t_upper, color="Upper band")) + 
        geom_line(data=df2, aes(x=x, y=predicted_x_t_lower, color="Lower band")) + 
      scale_colour_manual("", breaks = c("Actual y", "Predicted y", "Upper band", "Lower band"),
                        values = c("#000000", "#009E73", "#56B4E9", "#E69F00")) +
  ggtitle("Original vs. Predicted y with confidence bands")


ggplot() + 
  geom_line(data=df, aes(x=x, y=y, color="Actual y")) + 
    geom_line(data=df3, aes(x=x, y=y, color="Predicted y")) + 
        scale_colour_manual("", breaks = c("Actual y", "Predicted y"),
                        values = c("#000000","#56B4E9")) +
  ggtitle("Original vs. Predicted y using gausspr")

```


## Prediction Band

\fbox{\begin{minipage}{46.7em}
e) Generate $50$ observations from ARMA(1, 1) process with $\phi = 0.7$, $\theta = 0.50$. Use first $40$ values to fit an ARMA(1,1) model with $\mu = 0$. Plot the data, the $95\%$ prediction band and plot also the true $10$ values that you initially dropped. How many of them are outside the prediction band? How can this be interpreted?
\end{minipage}}


```{r}
x_t <- arima.sim(model = list(ar = c(0.7), ma=c(0.5)), n=50)
fit_x_t <- arima(x_t[1:40], order = c(1,0,1), include.mean = 0)

predicted_x_t <- predict(fit_x_t, n.ahead=10)
predicted_x_t_upper_band <- predicted_x_t$pred + 1.96 * predicted_x_t$se
predicted_x_t_lower_band <- predicted_x_t$pred - 1.96 * predicted_x_t$se

df <- data.frame(y = x_t[1:40], x=1:40) 
df2 <- data.frame(y = predicted_x_t$pred, 
                  upper_band=predicted_x_t_upper_band, 
                  lower_band=predicted_x_t_lower_band,
                  x = 41:50)

ggplot() + 
  geom_line(data=df, aes(x=x, y=y, color="Actual y")) + 
    geom_line(data=df2, aes(x=x, y=y, color="Predicted y")) + 
      geom_line(data=df2, aes(x=x, y=upper_band, color="Upper band")) + 
        geom_line(data=df2, aes(x=x, y=lower_band, color="Lower band")) + 
      scale_colour_manual("", breaks = c("Actual y", "Predicted y", "Upper band", "Lower band"),
                        values = c("#000000", "#009E73", "#56B4E9", "#E69F00")) +
  ggtitle("Original vs. Predicted y with confidence bands")  
```

## Finding a Suitable ARIMA Model and EACF

\fbox{\begin{minipage}{46.7em}
a) Find a suitable ARIMA(p, d, q) model for the data set `oil` present in the library `astsa`. Your modeling should include the following steps in an appropriate order: visualization, unit root test, detrending by differencing (if necessary), transformations (if necessary), ACF and PACF plots when needed, EACF analysis, Q-Q plots, Box-Ljung test, ARIMA fit analysis, control of the parameter redundancy in the fitted model. When performing these steps, always have 2 tentative models at hand and select one of them in the end. Validate your choice by AIC and BIC and write down the equation of the selected model. Finally, perform forecasting of the model $20$ observations ahead and provide a suitable plot showing the forecast and its uncertainty.
\end{minipage}}


```{r}
set.seed(12345)

# visualization
autoplot(ts(oil, start = 2000, frequency = 52)) + 
           ylab("Price of Oil") +xlab("Year") + 
           ggtitle("Price of Oil vs. Years")

ggAcf(oil) + ggtitle("ACF for Oil")
ggAcf(diff(oil)) + ggtitle("ACF for Oil with one diff")
ggPacf(oil) + ggtitle("PACF for Oil")
ggPacf(diff(oil)) + ggtitle("PACF for Oil with one diff")

# with log
autoplot(ts(log(oil), start = 2000, frequency = 52)) + 
           ylab("Price of Oil in Log") +xlab("Year") + 
           ggtitle("Price of Log Oil vs. Years")


autoplot(ts(diff(log(oil), lag=1), start = 1948, frequency = 12)) + 
           ylab("# Log Oil") +xlab("Year") + 
           ggtitle("Price of log oil with one lags vs. Years")

ggAcf(log(oil)) + ggtitle("ACF for log Oil")
ggAcf(diff(log(oil))) + ggtitle("ACF for log Oil with one diff")
ggPacf(log(oil)) + ggtitle("PACF for log Oil")
ggPacf(diff(log(oil))) + ggtitle("PACF for log Oil with one diff")


# EACF
eacf(diff(log(oil)))

```
Analysis: ARIMA(0,1,1) or ARIMA(1,1,1) or ARIMA(0,1,3) according to EACF


```{r, message=FALSE}

#Suggested Models
modelA <- sarima(log(oil), 0,1,1)
modelB <- sarima(log(oil), 1,1,1)
modelC <- sarima(log(oil), 0,1,3)

#ADF test
adf.test(modelA$fit$residuals)
adf.test(modelB$fit$residuals)
adf.test(modelC$fit$residuals)

#Redundancy check
summary(modelA$fit)
summary(modelB$fit)
summary(modelC$fit)

#BIC
BIC(modelA$fit)
BIC(modelB$fit)
BIC(modelC$fit)

#AIC
AIC(modelA$fit)
AIC(modelB$fit)
AIC(modelC$fit)

#Model C is the best
```
According to AIC and BIC the ModelC (ARIMA 0,1,3) is the best

Model equation is $\Delta x_t=w_t+0.1688w_{t-1}-0.0900w_{t-2}+0.1447w_{t-3}$

```{r}

#Forecasting
sarima.for(log(oil), 0,1,3, n.ahead = 20)

```


# State Space Models


In table 1 a script for generation of data from simulation of the following state space model and implementation of the Kalman filter on the data is given.


$$ Z_t =  A_{t-1} Z_{t-1} + e_t$$
$$x_t = C_t z_t + \nu_t$$

$$\nu_t \sim N(0,R_t)$$
$$e_t \sim N(0,Q_t)$$

\fbox{\begin{minipage}{46.7em}
a) Write down the expression for the state space model that is being simulated.
\end{minipage}}

$$ z_k = z_{k-1} + N(0,Q_t)$$
$$x_k = z_{k} + N(0, R_t)$$


\fbox{\begin{minipage}{46.7em}
b) Run this script and compare the filtering results with a moving average smoother of order 5.
\end{minipage}}

```{r}

# generate  dataset
set.seed(1)
num = 50
w = rnorm(num+1,0,1)
v = rnorm(num ,0,1)
mu = cumsum(w) # state: mu[0], mu[1],..., mu[50]
y = mu[-1] + v # obs: y[1],..., y[50]
# filter  and  smooth (Ksmooth0 does  both)
ks = Ksmooth0(num , y, A=1, mu0=0, Sigma0=1, Phi=1, cQ=1, cR=1)
# start  figurepar(mfrow=c(3,1))
Time = 1:num
plot(Time , mu[-1], main="Predict", ylim=c(-5,10))
lines(Time ,y,col="green")
lines(ks$xp)
lines(ks$xp+2*sqrt(ks$Pp), lty=2, col=4)
lines(ks$xp -2*sqrt(ks$Pp), lty=2, col=4)
plot(Time , mu[-1], main="Filter", ylim=c(-5,10))
lines(Time ,y,col="green")
lines(ks$xf)
lines(ks$xf+2*sqrt(ks$Pf), lty=2, col=4)
lines(ks$xf -2*sqrt(ks$Pf), lty=2, col=4)
plot(Time , mu[-1], main="Smooth", ylim=c(-5,10))
lines(Time ,y,col="green")
lines(ks$xs)
lines(ks$xs+2*sqrt(ks$Ps), lty=2, col=4)
lines(ks$xs-2*sqrt(ks$Ps), lty=2, col=4)
mu[1]
ks$x0n
sqrt(ks$P0n) # initial  value  info

# filering with moving average 5
plot(Time , mu[-1], ylim=c(-5,10), main="Moving average smoothing with order 5")
lines(Time ,y,col="green")
lines(ks$xf)
lines(ks$xf+2*sqrt(ks$Pf), lty=2, col=4)
lines(ks$xf -2*sqrt(ks$Pf), lty=2, col=4)
lines(ma(y, order=5, ), col=6)

```

Analysis: We find that the moving average smoothing function with order 5 is the worst fit since its losing all the variability that is captured by our kalman filter.

\fbox{\begin{minipage}{46.7em}
c) Also, compare the filtering outcome when R in the filter is 10 times smaller than its actual value, while Q in the filter is 10 times larger than its actual value. How does the filtering outcome varies?
\end{minipage}}

```{r}

# filter  and  smooth (Ksmooth0 does  both)
ks = Ksmooth0(num , y, A=1, mu0=0, Sigma0=1, Phi=1, cQ=10, cR=0.1)
# start  figurepar(mfrow=c(3,1))
Time = 1:num
plot(Time , mu[-1], main="Predict", ylim=c(-5,10))
lines(Time ,y,col="green")
lines(ks$xp)
lines(ks$xp+2*sqrt(ks$Pp), lty=2, col=4)
lines(ks$xp -2*sqrt(ks$Pp), lty=2, col=4)
plot(Time , mu[-1], main="Filter", ylim=c(-5,10))
lines(Time ,y,col="green")
lines(ks$xf)
lines(ks$xf+2*sqrt(ks$Pf), lty=2, col=4)
lines(ks$xf -2*sqrt(ks$Pf), lty=2, col=4)
```

Analysis: We find that the filtering output resembles the true value much more than the previousily run values.

\fbox{\begin{minipage}{46.7em}
d) Now compare the filtering outcome when R in the filter is 10 times larger than its actual value, while Q in the filter is 10 times smaller than its actual value. How does the filtering outcome varies?
\end{minipage}}

```{r}
# filter  and  smooth (Ksmooth0 does  both)
ks = Ksmooth0(num , y, A=1, mu0=0, Sigma0=1, Phi=1, cQ=0.1, cR=10)
# start  figurepar(mfrow=c(3,1))
Time = 1:num
plot(Time , mu[-1], main="Predict", ylim=c(-5,10))
lines(Time ,y,col="green")
lines(ks$xp)
lines(ks$xp+2*sqrt(ks$Pp), lty=2, col=4)
lines(ks$xp -2*sqrt(ks$Pp), lty=2, col=4)
plot(Time , mu[-1], main="Filter", ylim=c(-5,10))
lines(Time ,y,col="green")
lines(ks$xf)
lines(ks$xf+2*sqrt(ks$Pf), lty=2, col=4)
lines(ks$xf -2*sqrt(ks$Pf), lty=2, col=4)
```

Analysis: We find that the filtering output is far off the true value and hardly moving in terms of the predicted values, this is because kalman gain assumes that the uncertainity in measurement is high and it largely sticks with the mean of the series.

## Kalman Filter Code 

\fbox{\begin{minipage}{46.7em}
e) Implement your own Kalman filter and replace ksmooth0 function with your script.
\end{minipage}}

```{r}
#Times: Number of time steps
#A: How the mean of z_t will be affected by z_(t-1) Used in transition model
#C: Scale the mean (z_t) in the emission model
#Q: Covariate matrix in the emission model
#R: Covariate matrix in the transition model 
#y: All observations
#mu0: first mean
#sigma0: first varience
my_kalman <- function(Times, y, A, mu0, Sigma0, C, Q, R) {
  mu <- rep(1,Times)
  sigma <- rep(1,Times)
  my_unweighted <- rep(1,Times)
  sigma_unweighted <- rep(1,Times)
  kalman_gain <- rep(1,Times)
  
  # Our best guess is that my_1 is mu0 else it could be the first observation
  mu[1] <- mu0
  
  # We don't know what sigma_1 is, so we choose 1 else it would be provided
  sigma[1] <- Sigma0
  
  for (t in 2:Times) {
  # Calculate the unweighted prediction of the mean
  my_unweighted[t] <- A[t]%*%mu[t - 1]
    
  # Calculate the unweighted prediction of the covariate matrix
  sigma_unweighted[t] <- A[t]%*%sigma[t - 1]%*%t(A[t]) + Q[t]
    
  # Kalman gain Used to weight between our unweighted prediction and the obs
  kalman_gain[t] <- sigma_unweighted[t]%*%t(C[t]) %*% 
    solve(C[t]%*%sigma_unweighted[t]%*%t(C[t] + R[t]))
  
  # Calculate the weighted mean, thus our prediction of the hidden state
  mu[t] <- my_unweighted[t] + kalman_gain[t]%*%(y[t] - C[t]%*%my_unweighted[t])
    
    # Calculate the weighted covariance matrix, thus our prediction of the predition error
    sigma[t] <- (1 - kalman_gain[t]%*%C[t])%*%sigma_unweighted[t]
  }
  
  return (list(mu = mu, sigma = sigma))
}

```

### Alternative version
```{r}

### MODIFIED, DOES WORK

kalman_filter = function(num, data, A, C, Q, R, m0, P0) {
  
  if (length(A) == 1)
    A = as.list(rep(A, num+1))

  if (length(C) == 1)
    C = as.list(rep(C, num+1))
  
  if (length(Q) == 1)
    Q = as.list(rep(Q, num+1))
  
  if (length(R) == 1)
    R = as.list(rep(R, num+1))
  
  # Init
  m = list()
  P = list()
  K = list()
  
  # Setup
  # Note that the formula with the inverse has been changed, as otherwise the
  # dimensions have to be determined first.
  m[[1]] = m0
  P[[1]] = P0
  
  for (t in 2:(num)) {
    
    # Prediction Step
    m[[t-1]] = A[[t-1]] %*% m[[t-1]]
    P[[t-1]] = A[[t-1]] %*% P[[t-1]] %*% t(A[[t-1]]) + Q[[t]]
    
    # Observation Update Step
    K[[t]] = P[[t-1]] %*% t(C[[t]]) %*% solve(C[[t]] %*% P[[t-1]] 
                                              %*% t(C[[t]]) + R[[t]])
    m[[t]] = m[[t-1]] + K[[t]] %*% (data[[t]] - C[[t]] %*% m[[t-1]])
    P[[t]] = P[[t-1]] - K[[t]] %*% C[[t]] %*% P[[t-1]]
    #P[[t]] = (diag(ncol(K[[t]])) - K[[t]] %*% C[[t]]) %*% P[[t-1]]
    
  }
  
  return(list(m = unlist(m), P=unlist(P), K=unlist(K)))
}

# generate dataset
set.seed(1)
num = 50
w = rnorm(num+1, 0, 1)
v = rnorm(num, 0, 1)
mu = cumsum(w) # state: mu[0], mu[1],..., mu[50]
y = mu[-1] + v # obs: y[1],..., y[50]

# filter  and  smooth (Ksmooth0 does both)
ks = kalman_filter(num, y, A=1, m0=0, P0=1, C=1, Q=1, R=1)
# start figurepar(mfrow=c(3,1))
Time = 1:num

plot(Time, mu[-1], main="Predict (custom Kalman Filter)", ylim=c(-5, 10))
lines(Time, y, col="green")
lines(ks$m)
lines(ks$m+2*sqrt(ks$P), lty=2, col=4)
lines(ks$m-2*sqrt(ks$P), lty=2, col=4)

```



```{r}
# generate  dataset
set.seed(1)
num = 50
w = rnorm(num+1,0,1)
v = rnorm(num ,0,1)
mu = cumsum(w) # state: mu[0], mu[1],..., mu[50]
y = mu[-1] + v # obs: y[1],..., y[50]



my_ks = my_kalman(Times=num , y=y, A=rep(1,num), mu0=0, Sigma0=1, 
                  C=rep(1,num), Q=rep(1,num), R=rep(1,num))



plot(Time , mu[-1], main="Predict", ylim=c(-5,10))
lines(Time ,y,col="green")
lines(my_ks$mu)
lines(my_ks$mu+2*sqrt(my_ks$sigma), lty=2, col=4)
lines(my_ks$mu -2*sqrt(my_ks$sigma), lty=2, col=4)


```

\fbox{\begin{minipage}{46.7em}
f) How do you interpret the Kalman gain?
\end{minipage}}

Analysis: Kalman gain is given by $K=\frac{P_{k} H^T_{k}}{P_{k} H^T_{k} + R_k}$ where you will realize that the relative magnitudes of matrices $R_k$ and $P_k$ control a relation between the filter's use of predicted state estimate $z_t$ and measurement $x_t$.

When $R_k$ tends to zero then $x_t = x_{t-1} + K (y_t - H_k)$ suggests that when the magnitude of R is small, meaning that the measurements are accurate, the state estimate depends mostly on the measurements.

When the state is known accurately, then numerator is small compared to R, and the filter mostly ignores the measurements relying instead on the prediction derived from the previous state.

# Lecture Slides

## Lecture 1

\includepdf[pages={-}]{lecture1.pdf}

## Lecture 2

\includepdf[pages={-}]{lecture2.pdf}

## Lecture 3

\includepdf[pages={-}]{lecture3.pdf}

## Lecture 4

\includepdf[pages={-}]{lecture4.pdf}

## Lecture 5

\includepdf[pages={-}]{lecture5.pdf}

## Lecture 6

\includepdf[pages={-}]{lecture6.pdf}

## Lecture 7

\includepdf[pages={-}]{lecture7.pdf}

## Lecture 8

\includepdf[pages={-}]{lecture8.pdf}

## Lecture 9

\includepdf[pages={-}]{lecture9.pdf}

## Teaching Session II

\includepdf[pages={-}]{teaching_Session_II.pdf}

## Teaching Session III

\includepdf[pages={-}]{teaching_Session_III.pdf}

## Summary

\includepdf[pages={-}]{summary.pdf}

---
title: "732A90_ExamMaterial"
author: "Me"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    pdf_document:
      toc: true
header-includes:
  - \usepackage{booktabs} 
  - \usepackage{sectsty} \sectionfont{\centering}
  - \renewcommand{\contentsname}{}\vspace{-2cm} 
  - \usepackage{pdfpages}      
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
```

\newpage

#Libraries
```{r, message=FALSE}
library("ggplot2")
library("dplyr")
library("boot")
library("knitr")
```

# Distributions

## Relationship between distributions
```{r, echo=FALSE}
knitr::include_graphics(path="Relationships_among_distributions.jpg")
```

## Bernoulli Distribution

* PDF of Bernoulli

$$
\begin{split}
q = 1-p ~~if~k=0 \\
q = p~~if~k=1
\end{split}
$$
    + p is the probability of success
    + q is the probability of failure, q=1-p

* CDF of Bernoulli


$$
\begin{split}
    X(m,n) = \left\{\begin{array}{lr}
        0, & \text{if } k<1\\
        1-p, &\text{if } 0\leq k< 1\\
        1, & \text{if } k\geq 1
        \end{array}\right\}
\end{split}
$$

## Beta Distribution
```{r, echo=FALSE, out.width = "400px"}
knitr::include_graphics(path="beta_dist.png")
```

* PDF of Beta

$$\Gamma(n) = (n-1)! $$

$$ f(x;\alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}  x^{\alpha-1} (1-x)^{\beta-1} $$

## Exponential distribution

```{r, echo=FALSE, out.width = "400px"}
knitr::include_graphics(path="expo_dist.png")
```

* PDF of Exponential

    + lambda is the rate parameter 

$$ 
\begin{split}
    f(x;\lambda) = 
    \left\{\begin{array}{lr}
        \lambda e^{-\lambda \cdot x}, & \text{if } x\geq0\\
        0, &\text{if } x<0
        \end{array}\right\}
\end{split}
$$

* CDF of Exponential

$$ F(x;\lambda) = 1 - e^{-\lambda \cdot x}$$

## Pareto Distribution


```{r, echo=FALSE, out.width = "400px"}
knitr::include_graphics(path="pareto_dist.png")
```

* PDF of Pareto

    + xm is the (necessarily positive) minimum possible value of X
    + alpha is a positive parameter

$$ 
\begin{split}
    f(x) = 
    \left\{\begin{array}{lr}
        \frac{\alpha x_m^\alpha}{x^\alpha+1}, & \text{if } x\geq x_m\\
        0, &\text{if } x<x_m
        \end{array}\right\}
\end{split}
$$

* CDF of Pareto

$$ 
\begin{split}
    F(x) = 1 - (\frac{x_m}{x})^\alpha
\end{split}
$$

## Uniform Distribution

* PDF of Uniform

$$ 
\begin{split}
    f(x) = 
    \left\{\begin{array}{lr}
        \frac{1}{b-a}, & \text{if } a\leq x\leq b,\\
        0 &\text{otherwise } 
        \end{array}\right\}
\end{split}
$$


* CDF of Uniform

$$ 
\begin{split}
    F(x) = 
    \left\{\begin{array}{lr}
        0, & \text{if } x < a\\
        \frac{x-a}{b-a}, & \text{if } a\leq x \leq b \\
        1 &\text{otherwise } \\
        \end{array}\right\}
\end{split}
$$


## Normal Distribution

```{r, echo=FALSE, out.width = "400px"}
knitr::include_graphics(path="normal_dist.png")
```


* PDF of normal

    + mu is the mean or expectation of the distribution (and also its median and mode)
    + sigma is the standard deviation
    + sigma squares is the variance

$$ f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$
    
## Extra suggestions

Run the following to get different distributions and their formula
```{r, eval=FALSE}
help("Distributions")
```

# Random Sampling from Uniform distribution
## Sampling based probabilities proportional to the number of inhabitants of the city

```{r}
data <- read.csv2("population.csv", header = TRUE)
```


```{r}
get_city <- function(data){

data$Municipality <- as.character(data$Municipality)
data$prob <- data$Population/sum(data$Population)
#sorting the dataset
data <- data %>% arrange(prob)

data$cum_prob <- cumsum(data$prob)
data$lead_cum_prob <- lead(data$cum_prob, n=1)

# filling NA
data$lag_cum_prob[1] <- 0
data$lead_cum_prob[NROW(data)] <- 1

set.seed(12345)
num <- runif(1,0,1)

X <- ifelse(((num >= data$cum_prob) & (num <=  data$lead_cum_prob)), row.names(data), NA)
X <- na.omit(X)

data_name <- data[row.names(data) %in% X,]
return(data_name$Municipality)
}

```


```{r}

data$Municipality <- as.character(data$Municipality)
# Select one city
get_city(data = data)

# Remove one city
df <- data


df <- df[!df$Municipality %in% get_city(data=df),]

# apply function again
get_city(data = df)

# rempve this city
df <- df[!df$Municipality %in% get_city(data=df),]

# do this till 20 cities left
while(NROW(df) >20){
  df <- df[!df$Municipality %in% get_city(data=df),]
}

```
# Numeric Precision

Using a very small number making the numeric precision upto required digits

```{r}
options(digits = 22)
tol <- 1e-9
x1 <- 1/3
x2 <- 1/4

if(abs(x1-x2-1/12) <= tol){print("Subtraction is correct")
}else{print("Subtraction is wrong")}

```

# Random Number Generation
```{r}
x <- rep(0,10000)
x[1] <- 1

a <- 7^5
c <- 0
m <- 2^31-1

for(i in 1:length(x)){
  x[i+1] <- (a*x[i] + c) %% m
}

u <- x/m

hist(u)

```


## Implementing the Varience

```{r, warnings=FALSE, warnings=FALSE}

my_rand_num <- rnorm(n=10000, mean=10^8, sd=1)

myvar_better <- function(x){
  n <- length(x)
  m <- mean(x)
answer <- (1/(n - 1)) * sum((x - m)^2)
return(answer)
}

Y <- vector(mode = "numeric", length = length(my_rand_num))
my_mat2 <- vector(mode = "numeric", length = length(my_rand_num)+1)

for(i in 2:length(my_rand_num)){
  options(digits = 22)
  X_i = my_rand_num[1:i]
  X_i = na.omit(X_i)
  if((myvar_better(X_i) - var(X_i)) <= 1e-15){
    Y = 0
  }else{
      Y = myvar_better(X_i) - var(X_i)
      }
  temp <- cbind(i, Y)
  my_mat2 <- rbind(temp, my_mat2)
}

my_mat2 <- as.data.frame(my_mat2)

ggplot(my_mat2, aes(x=i, y=Y)) + geom_point() + ggtitle("Plot of Y vs. i")

```

# Scaling to get better results

The numeric precision of storage is highest between '0' and '1'

```{r, eval=FALSE}
kappa(A)
```

Analysis: Following the ?kappa function in R, we get a very large number for the condition number. A large value for the condition number indicates that this matrix is close to being singular.

```{r, eval=FALSE}

X_scale <- scale(X)
Y_scale <- scale(Y)

A_scale <- t(X_scale) %*% X_scale
b_scale <- t(X_scale) %*% Y_scale

B_hat_scale <- solve(A_scale, b_scale)

kappa(A_scale)

```

# Split the data into train and test

```{r}

data <- read.csv2("mortality_rate.csv")
data$LMR <- log(data$Rate)

n=NROW(data)
set.seed(123456)
id=sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]

```

# Loess Model with brute force of finding in minimizing of a function

```{r}

myMSE <- function(pars, lambda){

  X <- pars$X
  Y <- pars$Y
  Xtest <- pars$Xtest
  Ytest <- pars$Ytest
  
model <- loess(Y ~ X, enp.target=lambda)
predicted <- predict(model, Xtest)
n <- length(Ytest)

exp <- c()
for(i in 1:n){
  exp[i] <- (Ytest[i] - predicted[i])^2
}

answer_mse <- (1/n) * sum(exp)
     count <<- count + 1
return(answer_mse)
}

```


```{r, message=FALSE}
library(dplyr)

X <- train %>% select(c(Day)) %>% as.matrix() 
Y <- train %>% select(c(LMR)) %>% as.matrix() 

Xtest <- test %>% select(c(Day)) %>% as.matrix() 
Ytest <- test %>% select(c(LMR)) %>% as.matrix() 

pars <- list(X=X, Y=Y , Xtest=Xtest, Ytest=Ytest)

final <- NULL
count <- 0 
for(lambda in seq(from = 0.1, to = 40, by = 0.1)){
temp <- myMSE(pars, lambda=lambda)
temp <- cbind(temp, lambda)
final <- rbind(temp, final)
}

colnames(final) <- c("MSE", "lambda")

```


```{r}
ggplot(data=data.frame(final), aes(x = lambda, y=MSE)) + geom_point() + 
  ggtitle("Plot of MSE vs. Lambda")
```

# Optimize function to find the minimum

```{r}
count <- 0
optimize(interval = c(0.1, 40), f = myMSE, pars=pars, tol=0.01, maximum = FALSE)
cat("iternations: ", count)
```


# Optim function to find the minimum

## BFGS
```{r}
count <- 0
optim(35, myMSE, pars=pars, method = c("BFGS"))
cat("iternations: ", count)

```


## CG
```{r}
load("data.RData")
my_log_likehood <- function(pars){
  x<-data
  mu <- pars[1] 
  sigma <- pars[2]
  n <- length(x)
answer <- n*0.5*log(2*pi*sigma^2) + (0.5/sigma^2)* sum((x-mu)^2)  

return(answer)
}

gradient <- function(pars){
    x<-data
  mu <- pars[1] 
  sigma <- pars[2]
  n <- length(x)
  grad_mu <- - (1/sigma^2)* sum(x-mu)
  grad_sig <- (n/sigma) - (1/sigma^3) * sum((x-mu)^2)
  return(c(grad_mu, grad_sig))
}
```


## BFGS with and without gradient
```{r}
run1 <- optim(c(0,1), fn = my_log_likehood, gr=NULL, method = c("BFGS"))
run2 <- optim(c(0,1), fn = my_log_likehood, gr=gradient, method = c("BFGS"))
```

## CG with and without gradient
```{r}
run3 <- optim(c(0,1), fn = my_log_likehood, gr=NULL, method = c("CG"))
run4 <- optim(c(0,1), fn = my_log_likehood, gr=gradient, method = c("CG"))
```

## Showing all results in table

```{r}
final <- NULL
final$algorithm <- c("BFGS", "CG", "BFGS+gradient", "CG+gradient")
final$parameters <- rbind(run1$par, run2$par, run3$par, run4$par)
final$counts <- rbind(run1$counts, run2$counts, run3$counts, run4$counts)
final$convergence <- rbind(run1$convergence, run2$convergence, run3$convergence, run4$convergence)
final$value <- rbind(run1$value, run2$value, run3$value, run4$value)


knitr::kable(as.data.frame(final), caption = "Table showing the summary from various optimization techniques")
```

# Inverse CDF


The inverse transform method is a simple algorithm for generating random variables $x$ from a *continuous* target distribution $f(x)$ using random samples from a $Unif(0,1)$ distribution.

Algorithm:

1. For target probability density function (*pdf*) $f(X)$, calculate the CDF, $F(X)$

2. Set the CDF equal to $U$, $F(X) = U$, then solving for $X$, obtaining $F^{-1}(U) = X$

2. Generate $n$ random variables from $u \sim Unif(0,1)$

3. Plug in $u$ observed values in $F^{-1}(U = u)$ to obtain $n$ values for which $x \sim f(X)$


## Example: Pareto Distribution

For information on the Pareto distribution.

The $Pareto(a,b)$ distribution has CDF $F(X \leq x) = 1 - (\frac{b}{x})^a$ for $x \geq b > 0, \ a > 0$


1. First set $F(x) = U$, where $U \sim Unif(0,1)$, then solve for $X$
$$
\displaystyle
\begin{aligned}
1 - \left( \frac{b}{x} \right)^2 & = U \\
\ \left(\frac{b}{x} \right)^a & = 1 - U \\
\  \frac{b}{x} & = (1 - U)^{1/a} \\
\ x & = b \times (1 - U)^{-1/a} \\
\ & = F_X^{-1}(U) \\
\end{aligned}
$$

```{r}
set.seed(123)
n = 1000
U =runif(n)
a = 3
b = 2
X = b*(1-U)^(-1/a)
pareto = function(x){(a*(b^a)/x^(a+1))}

summary(X)

```


```{r}
hist(X, probability = TRUE, breaks = 25, xlim =c(0, 20),
     col = "gray", border = "white",
     main = "Inverse Transform: Pareto(3,2)", xlab = "x")
curve(pareto(x), from = 0, to = 40, add = TRUE, col = "blue")

```



##The double exponential (Laplace) distribution is given by formula:

$$ DE(\mu, \alpha) = \frac{\alpha}{2} e^{(-\alpha|x - \mu|)} $$

CDF:
$$F(x)=\int_{-\infty}^{x} f(x) dx$$

$$F(x)=\int_{-\infty}^{x} \frac{\alpha}{2}e^{-\alpha(x-\mu)} dx ,~~~~~(if~~x>\mu)$$
$$=1-\int_{x}^{\infty} \frac{\alpha}{2}e^{-\alpha(x-\mu)} dx$$

$$=1-\frac{1}{2}e^{-\alpha(x-\mu)}$$

$$F(x)=\int_{-\infty}^{x} \frac{\alpha}{2}e^{\alpha(x-\mu)} dx ,~~~~~(if~~x\leq\mu)$$

$$=\frac{1}{2}e^{\alpha(x-\mu)}$$
Inverse of CDF

$$For~~x>\mu,~we~got~F(x)=1-\frac{1}{2}e^{-\alpha(x-\mu)}$$
$$y=1-\frac{1}{2}e^{-\alpha(x-\mu)}$$
$$\frac{ln(2-2y)-\alpha\mu}{-\alpha}=x$$
$$For~U\sim U(0,1),~~~~~\frac{ln(2-2U)-\alpha\mu}{-\alpha}=X$$
$$For~~x\leq\mu,~we~got~F(x)=\frac{1}{2}e^{\alpha(x-\mu)}$$
$$y=\frac{1}{2}e^{\alpha(x-\mu)}$$
$$\frac{ln(2y)}{\alpha}+\mu=x$$
$$For~U\sim U(0,1),~~~~~\frac{ln(2U)}{\alpha}+\mu=X$$

```{r}

de_dist <- function(u, mu, a){
  de_distribution <- c()

    for (i in 1:length(u)){
      if (u[i] > 0.5){
        de_distribution[i] <- (log(2-2*u[i])-a*mu)/(-a)
      }
      else {
        de_distribution[i] <- (log(2*u[i])/a)+mu
      }
    }
  return(de_distribution)
}
```

```{r}
new_distribution <- de_dist(runif(10000, 0, 1), mu=0, a=1)
hist(new_distribution, breaks = 1000)
```


# Generate Normal distribution using uniform

```{r}

box_muller <- function(n = 1, mean = 0, sd = 1)
{
  x <- vector("numeric", n)

  i <- 1
  while(i <= n)
  {
    u1 <- runif(1, 0, 1)
    u2 <- runif(1, 0, 1)

    x[i] <- sqrt(-2 * log(u1)) * cos(2 * pi * u2)

    if ((i + 1) <= n)
    {
      x[i + 1] <- sqrt(-2 * log(u1)) * sin(2 * pi * u2)
      i <- i + 1
    }

    i <- i + 1
  }

  x * sd + mean
}

hist(box_muller(1000))

```

# Accpetance/Rejection Method


To generate $n$ samples, `for(i in 1:n)`

  1. Generate $Y \sim g_Y(t)$ and $U \sim Unif(0,1)$

  2. If $U \leq \frac{f(Y)}{M \times g(Y)}$ then we accept $Y$, such that $Y =  X$

  3. Repeat until you have sufficient samples



In order for the algorithm to work we require the following constraings:

1. $f$ and $g$ have to have compatible supports (i.e. $g(x) > 0$ when $f(x) > 0$)

2. There is a constant $M$ such that $\frac{f(t)}{g(t)} \leq M$


## Example: Beta(2,2)

Suppose we'd like to generate samples from $Beta(2,2)$ distribution. The density function for $Beta(2,2)$ is simply $f(x) = 6x(1 - x)$ for $0 < x < 1$. Since our domain is between 0 and 1, we can use a simple $Unif(0,1)$ density as our instrumental density, $g$. Then, by the accept-reject algorithm we can simulate a random variable $Y \sim g$, and a random variable $U \sim Unif(0,1)$. Then, if 
$$
M \times U \leq \frac{f(Y)}{ g(Y)}
$$
we accept the candidate variable $Y \sim g$ as $X$, $X = Y$. Otherwise, we reject $Y$ and simulate again until we get an appropriate sample size. 


```{r, echo = FALSE, fig.height=3, fig.width=9}

par(mfrow = c(1,2), mar = c(3, 4, 1, 2))
beta <- function(x) 6*x*(1-x)
M = 1.5
## plot 2
curve(expr = beta, from = 0, to = 1, 
      xlim = c(-0.5, 1.5), ylim = c(0,2),
      main = "Beta(2,2) Density with Unif(0,1)",
      xlab = "x", ylab = "density")

x = seq(from = -1, to = 2, by = 0.01)
Unif1 = function(x){ ifelse(x >= 0 & x <= 1, 1, 0) }
polygon(x, Unif1(x), lty = 9)

## plot 3
curve(expr = beta, from = 0, to = 1, 
      xlim = c(-0.2, 1.2), ylim = c(0,2),
      main = "Beta(2,2) with M*Unif(0,1)",
      xlab = "x", ylab = "density")

Unif2 = function(x){ ifelse(x >= 0 & x <= 1, 1*M, 0) }
polygon(x, Unif2(x), lty = 2)

#abline(h = 1.5, col = "red")
par(mfrow=c(1,1))


```


### Generating N points and finding maximizing value


Note that the target density $f$ has a maximum of 1.5, so we can set M = 1.5 (using calculus or use optimize function as shown)

```{r}

# find M using optimize
f <- function(x){ 6*x*(1 - x)} ## pdf of Beta(2,2)
max_find <- optimize(f=f, lower = 0, upper = 10, maximum = TRUE)
max_find

M <- max_find$objective



## Accept-Reject
M = 1.5
X = rep(x = NA, 5) ## create a vector of length 5 of NAs
set.seed(123)
f <- function(x){ 6*x*(1 - x)} ## pdf of Beta(2,2)
g <- function(x){ 1 } ## pdf of Unif(0,1) is just 1

n = 10000

```

Now, say we needed $n = 10,000$ samples from $Beta(2,2)$, then a better implementation would be

```{r}
X = rep(NA, n); M = 1.5
i = 0 ## index set to start at 0
while(sum(is.na(X))){
  U = runif(1); Y = runif(1)
  accept <- U <= f(Y)/(M*g(Y))
  if(accept){
    i = i+1 ## update the index
    X[i] <- Y
  }
}

round(summary(X), 4)
round(qbeta(p = c(0, 0.25, 0.5, 0.75, 1), 2, 2), 4)
```


## Example

$$ \frac{f(y)}{g(y)} \leq c $$

Thus if we maximize the ratio f(y)/g(y) then that would be the value of c. This can be done by using parital derivate of the fraction.


$$ Majorizing~density~F_{Y}(y)\sim DE(0,1)=\frac{1}{2}e^{-|x|} $$

$$ Target~density~F_{X}(y)\sim N(0,1)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$$

$$ C \ge \frac{\sqrt{2}}{\sqrt{\pi}} e^{-\frac{x^2}{2} + |y|} $$
Differenitating with respect to x we get that the expression is maximum at x=1, thus C is:

$$ \sqrt{\frac{2}{\pi}}e^\frac{-1}{2}$$

$$=\frac{\sqrt{2}}{C\sqrt{\pi}}\cdotp e^{-\frac{y^2}{2}+|y|}$$
$$ = e^{-\frac{y^2}{2}+|y|+\frac{1}{2}} $$


Thus C is

$$ C = \sqrt(\frac{2*e^1}{\pi})$$   


```{r}
generate_n <- function(c){
  x <- NA
  num.reject <- 0
    while (is.na(x)){
      u <- runif(1, 0, 1)
      y <- de_dist(u, 0, 1)
      U <- runif(1)
        if (y>0 && U <= sqrt(2/pi)/c*exp(-(y^2)/2)+y){
          x <- y
        } else if (y<=0 && U<=sqrt(2/pi)/c*exp(-(y^2)/2-y)){
            x <- y
              } else {
                num.reject <- num.reject + 1
            }
    }
  c(x,num.reject)
}


c <- sqrt(2*exp(1)/pi)

```


```{r}
set.seed(12345)
nnormal <- sapply(rep(c,2000), generate_n)
```

```{r}
hist(nnormal[1,], breaks = 50, freq = FALSE, xlim = c(-4,4),
     main = "Normal distribution generated by Accept/Reject method")
lines(density(nnormal[1,]))
```

```{r}
average_rejection <- sum(nnormal[2,])/(ncol(nnormal)+sum(nnormal[2,]))
average_rejection
```

```{r}
expected_rejection <- 1-(1/c)
expected_rejection
```

```{r}
rejection_difference <- expected_rejection - average_rejection
rejection_difference
```

```{r}
newnormal <- nnormal[1,]
random <- rnorm(2000, 0, 1)
df <- as.data.frame(cbind(newnormal, random))
```

```{r}

ggplot(df, aes(newnormal, fill = "newnormal")) + geom_histogram(alpha = 0.4, bins = 50) +
  geom_histogram(aes(random, fill = "random"), alpha = 0.4, bins = 50)

```


# Metropolis Hastings 


The **Independent Metropolis-Hastings algorithm** as described Robert & Casella goes as follows

Given $x^{(t)}$

`1. Generate` $Y_t \sim g(y)$

`2. Take`

$$
X_{t+1} = 
  \begin{cases}
    Y_t       & \quad  \text{with probability }\ \rho(x^{(t)}, Y_t) \\
    x^{(t)}   & \quad  \text{with probability }\ 1 - \rho(x^{(t)}, Y_t) \\
  \end{cases}
$$
where 
$$
\displaystyle \rho(x^{(t)}, Y_t) = \text{min} \left\{ \frac{f(Y_t)}{f(x^{(t)})} \frac{g(x^{(t)})}{g(Y_t)}, 1 \right\}
$$




### Independent M-H

In simpler terms, as we want to generate $X \sim f$, we first take an initial value $x^{(0)}$ (which can almost be any artibrary value in the support of $f$). 

1. We generate a value $Y_0 \sim q(y | x^{(0)})$. 
2. We calculate $\rho(x^{(t)}, Y_t)$
3. Generate a random value $U \sim Unif(0,1)$ 
4. If $U < \rho(x^{(t)}, Y_t)$, then we accept $X^{(1)} = Y_t$;
else we take $X^{(1)} = X^{(0)}$
5. Repeate steps 1-4 until you've satisfied the number of samples needed








### Example
Use Metropolis-Hastings algorithm to generate samples from the below distribution by using proposal distribution as chi^2(floor(X(t)+1)), take some starting point. Plot the chain you obtained as a time series plot. What can you guess about the convergence of the chain? If there is a burn-in period, what can be the size of this period?

$$ f(X) \sim  x^5 e^{-x}, x>0$$

```{r}
#density function from which we want to sample.
dt <- function(x) {
  if (x <= 0) {
    stop("x has to be greater than 0.")
  }
  return(x^5 * exp(-x))
}

#proposal density function, should accept 2 arguments:
  #   x:  value at which to compute density.
  #   x_t: value on which the rq is conditioned.
dp <- function(x, xt) {
  dchisq(x, floor(xt + 1))
}

rp <- function(x) {
  rchisq(1, floor(x+1))
}

metropolis_hastings <- function(x_0, t_max, dt, dp, rp) {
  # Perform Metropolis-Hastings sampling of the specified density.
  # 
  # Args:
  #   x_0   starting value.
  #   t_max maximum numer of iterations.
  #   dt    density function from which we want to sample.
  #   dp    proposal density function, should accept 2 arguments:
  #           x:  value at which to compute density.
  #           x_t: value on which the rq is conditioned.
  #   rp    proposal random number generator, should accept 1 argument: 
  #           x_t: value on which the rq is conditioned.
  #           
  # Returns:
  #   Vector of sampled points of length t_max. 
  
  x_t <- x_0
  x <- vector("numeric", t_max) # pre-allocate
  
  # Metropolis-Hastings
  for (t in 1:t_max) {
    # Generate a candidate
    y <- rp(x_t)
    # Generate U
    u <- runif(1, 0, 1)
    # Compute alpha
    alpha <- min(1, (dt(y) * dp(x_t, y)) / (dt(x_t) * dp(y, x_t)))
    # Accept the jump or stay in x_t
    if (u < alpha) {
      x_t <- y
    }
    # Sace x_t in vector
    x[t] <- x_t
  }
  
  return(x)
}


test <- metropolis_hastings(x_0=1, t_max = 10000, dt, dp, rp)

# compare the distribution
x=seq(from=0.01, to=20, by=0.002)
actual <- numeric(length(x))
for(i in seq_along(x)){
  actual[i] <- dt(x=x[i])
}

quantile(test)
quantile(temp)


ggplot() +
geom_line(aes(x=1:10000, y=test)) +
labs(x="Iterations", y="X") + 
ylim(0, 50) +
ggtitle("Metropolis-Hasting Sampler using Chisquare")


```

# Gelman Rubin
```{r, eval = TRUE}

all_series <- NULL
for(i in 1:10) { 
temp <- metropolis_hastings(x_0=i, t_max = 10000, dt, dp, rp)
temp <- coda::as.mcmc(temp)
all_series[[i]] <- temp
}

#convergence analysis
coda::gelman.diag(all_series)
```


# Gibbs Sampling

```{r}
calculate_conditional_probability <- function(i, X, Y, ...) {
  # Returns the conditional probability of an element
  # Args: (might change with the given task)
  #   X   Value we are looking for
  #   Y   Data
  #   i   position
  # In the lab we had to consider three cases 
  # This will change in the exam!
  d <- length(X)
  if (i == 1) {
    # i = 1
    mean <- (Y[1] + X[2]) / 2
    variance <- sigma_squared / 2
  } else if (i == d) {
    # i = d
    mean <- (X[d - 1] + Y[d]) / 2
    variance <- sigma_squared / 2
  } else { 
    # i = 2 ... d-1
    mean <- (X[i - 1]+ Y[i] + X[i + 1]) / 3
    variance <- sigma_squared / 3
  }
  
  # Generate a random variables
  X_i <- rnorm(1, mean = mean, sd = sqrt(variance))
  
  return(X_i)
}

gibbs_sampling <- function(Y, n, X0, ...) {
  # Gibbs sampling
  # Args:
  #   Y   Data
  #   n   Number of generated samples
  #   X0  Initialization points
  # Returns:
  #   X   Matrix with samples (colmeans = value we are looking for)
  d <- length(Y)
  X <- matrix(NA, ncol = d, nrow = n)
  X[1, ] <- X0
  
  for (row in 2:n) {
    for (col in 1:d) {
      X[row, col] <- calculate_conditional_probability(
        i = col,
        X = c(X[row, 1:(col - 1)], X[row - 1, col:d]),
        Y = Y,
        sigma_squared = sigma_squared
      )
    }
  }
  return(X)
}


```


# Bootstrap estimation

## Estimating the distribution of T by using a non-parametric bootstrap with B = 2000

### Using Boot library
$T = \frac{\hat{Y}(X_b)-\hat{Y}(X_a)}{X_b - X_a}$ Where $X_b = argmax_xY(X)$ and $X_a = argmin_xY(X)$

```{r}
data <- read.csv2('lottery.csv')

stat1 <- function(data,vn){
  data <- as.data.frame(data[vn,])
  X_b <- data[which.max(data$Draft_No),]$Day_of_year
  X_a <- data[which.min(data$Draft_No),]$Day_of_year
  model <- loess(formula = Draft_No~Day_of_year
               , data = data)
  Y_hat <- predict(object = model, newdata = data)
  Y_hat_a <- Y_hat[X_a]
  Y_hat_b <- Y_hat[X_b]
  (Y_hat_b - Y_hat_a)/(X_b - X_a)
}

res <- boot(data = data, statistic = stat1
                        # Ordinary nonparametric bootstrap (default)
            , R = 2000, sim = "ordinary")


plot(res)

# P-Value
mean(abs(res$t0) < abs(res$t-mean(res$t)))

```


### Without Boot library

```{r}
set.seed(12345)

X <- data$Day_of_year
Y <- data$Draft_No

T <- numeric()

for (i in 1:2000) {
        boot <-sample(length(X), replace=TRUE)
        data1 <- cbind(X,boot) 


        X_a <- data1[which.min(data1[,2])]
        X_b <- data1[which.max(data1[,2])]
        
        model2 <- loess(Y~X, data=as.data.frame(data1), method="loess")
        
        fitted_X_a <- model2$fitted[X_a]
        fitted_X_b <- model2$fitted[X_b]
        
        test <- (fitted_X_b - fitted_X_a)/(X_b - X_a)
        T[i] <- test
        }


pval <- length(which(T>=0))


hist(T, breaks=30,
     main="Distribution of T by using non-parametric bootstrapping")

print("Estimated p-value:")
print(pval/2000)

```


## Varience Estimate

###Estimate the distribution of the mean price of the house using bootstrap. Determine the bootstrap bias-correction and the variance of the mean price. Compute a 95% confidence interval for the mean price using bootstrap percentile, bootstrap BCa, and first-order normal approximation.

Bias correction
$$T1 = 2.T(D) - \frac{1}{D} \sum_{i=1}^B T^*_{i} $$

```{r}

price_data <- read.csv("prices1.csv", sep=";")

# Estimation of mean of Price
stat_mean <- function(data, index){
    data <- data[index,]
    answer <- mean(data$Price)
    return(answer)
}

res <- boot::boot(data=price_data, statistic = stat_mean, R=2000)
res
plot(res,index = 1)

#95% CI for mean using percentile
boot.ci(res, index=1, type=c('perc'))

#95% CI for mean using bca
boot.ci(res, index=1, type=c('bca'))

#95% CI for mean using first order normal
boot.ci(res, index=1, type=c('norm'))
```


```{r}
# Bias-correction and Varience of Price
boot.fn <- function(data,index){
        d <- data[index]
        res <- mean(d)
}

boot.result <- boot(data=price_data$Price, statistic=boot.fn, R=1000) 
bias_cor <- 2*mean(price_data$Price)-mean(boot.result$t)


list("bias_correction"=bias_cor, "variance_of_the_mean_price"=35.93^2)
boot.result
plot(boot.result)
boot.ci(boot.result)


```

### Estimate the variance of the mean price using the jackknife and compare it with the bootstrap estimate
**Jacknife(n=B)**:
$$\widehat{Var[T(\cdot)]}=\frac{1}{n(n-1)}\sum_{i=1}^{n}(T_{i}^*-J(T))^2$$
where,
$$T_{i}^*=nT(D)-(n-1)T(D_i^*)~,~~~~J(T)=\frac{1}{n}\sum_{i=1}^{n}T_{i}^*$$
When you compute the equation given aboce, you got
$$\frac{n-1}{n}\sum_{i=1}^{n}(T_{i}^*-J(T))^2$$
Reference:The Jackknife Estimation Method, Avery I. McIntosh (http://people.bu.edu/aimcinto/jackknife.pdf)
```{r}
result <- numeric()
n <- NROW(price_data)

for (i in 1:n){
        updated_price <- price_data$Price[-i]
        result[i] <- mean(updated_price)
}

var_T <- (n-1)/n*sum((result-mean(result))^2)
mean_T <- mean(result)

cat("The variance from jacknife method is:", var_T)
```
Analysis: The obtained variance using Jackknife method is 1320.911 while using bootstrapping the obtained value was 1290.965. Considering the fact that Jackknife overestimate variance, the answer seems reasonable. 

### Compare the confidence intervals obtained with respect to their length and the location of the estimated mean in these intervals.

```{r}

confidence_interval_jackknife <- c((mean_T - 1.96*var_T), (mean_T + 1.96*var_T))

confidence_interval_jackknife

intervals <- c("(1150-1011)","(1148-1011)","(1149-1013)","(1146-1007)")
length <- c(1150-1011, 1148-1011,1149-1013,1146-1007)
Center_of_interval <- c((1150+1011)/2,(1148+1011)/2,(1149+1013)/2,(1146+1007)/2)
 
dt <- data.frame(intervals, length, Center_of_interval,row.names = c("Normal", "Basic", "Percentile", "BCa"))

dt %>% kable(col.names = c("Confidence interval", "Length of interval","Center of interval"))

```

# Integration Help
\includepdf[pages={-}]{integral_rules.pdf}


# Lecture slides

\includepdf[pages={-}]{lecture_slides.pdf}



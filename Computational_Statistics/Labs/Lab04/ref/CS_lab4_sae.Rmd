---
title: "Computational Statistics Lab04"
author: "Saewon Jun(saeju204)"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1: Computations with Metropolis-Hastings
Consider the following probability density function:
$$f(x)\propto x^5 e^{-x},~~~x>0$$
You can see that the distribution is known up to some constant of proprotionality. This constant can be found by applying integration by parts multiple times and equals 120.
  
###1.1 Metropolis-Hastings algorithm
Use Metropolis-Hastings algorithm to generate samples from this distribution by using proposal distribution as *log-normal* $LN(X_t,1)$, take some starting point. Plot the chain you obtained as a time series plot.
- PDF $\pi(x)$ that we want to sample from
- Proposal distribution $q(\cdot|X_t)$ that has a regular form with respect to $\pi(\cdot)$
  E.g. $q(\cdot|X_t)$ is normal with mean X_t and given variance
- Regular form: suffices that the proposal has the same support as $\pi$.

```{r}
set.seed(12345)

pi <-function(x){
        x^5*exp(-x)
} #얘가 목표 분포

f.MCMC.MH<-function(nstep,X0,props,plot=TRUE,return=FALSE){
    vN<-1:nstep
    vX<-rep(X0,nstep) #initializing the chain
    
    for (i in 2:nstep){
	X<-vX[i-1]
	Y<-rlnorm(1,mean=X,sd=props) #proposal distribution
	u<-runif(1)
	a<-min(c(1,(pi(Y)*dlnorm(X,mean=Y,sd=props))/
	           (pi(X)*dlnorm(Y,mean=X,sd=props))))
	
	if (u <= a){
	        vX[i]<-Y
	} else {
	        vX[i]<-X }    
    }
    
     if(return==TRUE){
            return(vX)
     }
    
    
    if(plot==TRUE){
    plot(vN,vX, pch=19, cex=0.3, col="black", xlab="t", ylab="X(t)",
            main="", ylim=c(min(X0-0.5,-5),max(5,X0+0.5)))
    abline(h=0)
    abline(h=1.96)
    abline(h=-1.96)
    }
}
```

```{r}
set.seed(12345)
X0 <- rlnorm(1,0,1)
props <- 1
f.MCMC.MH(5000,X0,props)
```
What can you guess about the convergence of the chain? If there is a burn-in period, what can be the size of this period? 

Burn-in period is, at the beginning of an MCMC run, you throw away some iterations (nstep). After the burn-in you run normally, using eaxh iterate in your MCMC calculation.

The most of the data points are not included in the boundary. So we can't say this Markov chain is stationary, and we can't find any burn-in period either. The chain seems to be not converging. 

###1.2 using chi-squre distribution $X^2(\lfloor X_t +1 \rfloor)$ as a proposal distribution. 
```{r}
set.seed(12345)

chi <- function(x){
       rchisq(1,floor(x+1))
}



f.MCMC.MH_2<-function(nstep,X0,plot=TRUE,return=FALSE){
    vN<-1:nstep
    vX<-rep(X0,nstep) #initializing the chain
    
    for (i in 2:nstep){
	X<-vX[i-1]
	Y<-rchisq(1,floor(X+1)) #proposal distribution
	
	u<-runif(1)
	a<-min(c(1,(pi(Y)*dchisq(X,Y)/
	           (pi(X)*dchisq(Y,X)))))
	
	if (u <= a){
	        vX[i]<-Y
	} else {
	        vX[i]<-X }    
    }
     
    if(return==TRUE){
            return(vX)
    }
    
    if(plot==TRUE){
    plot(vN,vX, pch=19, cex=0.3, col="black", xlab="t", ylab="X(t)",
         ylim=c(0,25))
    abline(h=0)
    abline(h=1.96)
    abline(h=-1.96)
    }
    
    
}

```

```{r}
set.seed(12345)
f.MCMC.MH_2(5000,rchisq(1,floor(1+1)),plot=TRUE,return=FALSE)
```
  
###1.3 Compare the result of steps1 and steps2, make conclusion
using chi-square as a proposal distribution seems more stable ....????

###1.4 Generate 10 MCMC sequences using the generator from Step 2(chi-square) and starting points 1,2,3,.. or 10. Use the Gelman-Rubin method to analyze convergence of these sequences.
```{r}
set.seed(12345)

seq_mean<-c()
mean_sum <- 0
seq_sdev <- c() ##(data point)-(sequence mean)

n <- 5000 #length of sequence
k <- 10 #10 different sequence

#Now caculate the mean for each sequence
for (i in 1:10){
        point <- f.MCMC.MH_2(n,rchisq(1,floor(i+1)),plot=FALSE,return=TRUE)
        seq_mean[i] <- mean(point)
        mean_sum <- mean_sum + mean(point)
        seq_sdev[i]<-sum((point-seq_mean[i])^2)
        
        
}

g_mean <- mean_sum/k ##global mean

#Now calculate the between/within sequence variance
s_sq <- seq_sdev/(n-1)
B <- n/(k-1)*sum(seq_mean-g_mean)
W <- sum(s_sq)/k

#compute Gelman-Rubin factor
R <- ((n-1)/n*W + B/n)/W

R
```
Given that values much larger than 1 indicate the lack of convergence, the Gelman-Rubin factor we got (0.9998) indicates that the seqeunce are converging.

###1.5 Estimate 
$$\int_{0}^{\infty}xf(x)~dx$$
using the samples from Steps 1 and 2. 
Equation above equals to the mean value, so we simply calculated the mean value as following :
####1.5.1 using the sample from **log-normal** $LN(X_t,1)$ 
```{r}
sample1 <- f.MCMC.MH(5000,X0,props,plot=FALSE,return=TRUE)
mean(sample1)
```

####1.5.2 using the sample from **chi-square distribution** $\chi^2(\lfloor X_t +1 \rfloor)$
```{r}
sample2 <- f.MCMC.MH_2(5000,rchisq(1,floor(1+1)),plot=FALSE,return=TRUE)
mean(sample2)
```
  
###1.6 The distribution generated is in fact a gamma distribution. Look in the literature and define the actual value of the integral. Compare it with the one you obtained.
  
https://enginius.tistory.com/514
  
  
## Question 2: Gibbs sampling
A concentration of a certain chemical was measured in a water sample, and the result was stored in the data chemical.Rdata having the following variables:
  
- X : day of the measurement
- Y : measured concentration of the chemical
  
The instrument used to measure the concentratoin had certain accuracy; this is why the measurements can be treated as noisy. Your purpose is to restore the expected concentration values.
  
###2.1 Import the data and plot the dependence of Y on X. What kind of model is reasonable to use here?
```{r}
load("chemical.RData") #you can check the list of data with ls()
data <- data.frame(X,Y)

plot(X,Y, main="dependence of Y on X", xlab="Day of the measurement",
     ylab="Measured concentration of the chemical", type="o",
     pch=19, cex=0.5, col="black")
```
Polynomial regession mdoel seems to be reasonable for *X*:Day of the measurement and *Y*:Measureed concentration of the chemical.

###2.2 A researcher has decided to use the following (random-walk) Bayesian model:
- n: number of observations
- $\vec{\mu}=(u_1,...,\mu_n)$ are unknown parameters:
$$Y_i\sim N(\mu_i, variance=0.2),~i=1,...,n$$
where the prior is 
$$p(\mu_1)=1$$
$$p(\mu_{i+1}|\mu_i)=N(\mu_i,0.2),~i=1,...,n1$$
Present the formulae showing the liklihood $p(\vec{Y}|\vec{\mu})$ and the prior $p(\vec{\mu})$.
  
Given $Y_i \sim N(\mu_i,variance=0.2)$,
$$f_{\mu_i,\sigma^2=0.2}(y_i)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\bigg(\frac{-(y_i-\mu_i)^2}{2\sigma^2}\bigg)$$
$$liklihood~~p(\vec{Y}|\vec{\mu})=\prod f_{\mu,\sigma}(y_i)=\prod \frac{1}{\sqrt{0.8\pi}}exp\bigg(\frac{-(y_i-\mu_i)^2}{0.4}\bigg)$$
$$\propto exp\bigg(\sum_{i}^{n}\frac{-(y_i-\mu_i)^2}{0.4}\bigg)$$
  
  
Given condition for prior from above and with the bayesian chain rule as following:
$$p(\vec{\mu})=p(\vec{\mu_1})p(\vec{\mu_2}|\vec{\mu_1})p(\vec{\mu_3}|\vec{\mu_2})...p(\vec{\mu_n}|\vec{\mu_{n1}})$$
we just simply multiply each given probabilities, we get
$$p(\vec{\mu})\propto exp\bigg(\sum_{i}^{n1}\frac{-(\mu_i-\mu_{i-1})^2}{0.4}\bigg)$$
  
###2.3Use Bayes' Theorem to get the posterior up to a constant proportionality, and then find out the distribution of $(\mu_i|\vec{\mu_{-i}},\vec{Y})$, where $\vec{\mu_{-i}}$ is a vector containing all \mu values except for $\mu_i$.
To get the poesterior with given data D(up to d dimension), we follow following definition:
$$P(Y|D)=P(Y_{(i)}^{(t)}|D,Y_{1}^{(t+1)},...,Y_{(i-1)}^{(t+1)},Y_{(i+1)}^{(t)},...,Y_{d}^{(t)}$$
The distribution of $(\mu_i |\vec{\mu_{-i}},\vec{Y})$ can be computed by 
$$p(\mu_i |\vec{\mu_{-i}},\vec{Y})=p(\vec{Y}|\vec{\mu})\times p(\vec{\mu})$$
$$\propto exp\bigg(\sum_{i}^{n}\frac{-(y_i-\mu_i)^2}{0.4}\bigg)\times exp\bigg(\sum_{i}^{n1}\frac{-(\mu_i-\mu_{i-1})^2}{0.4}\bigg)$$
$$\propto exp\bigg(\sum_{i}^{n}\frac{-(\mu_i-y_i)^2-(\mu_i-\mu_{i-1})^2}{0.4}\bigg)$$
$$\propto exp\bigg(-\frac{(\mu_i-(y_i+\mu_{i-1})/2)^2}{0.4/2}\bigg)$$
  
###2.4 Implement Gibbs Sampler
Use the distributions derived in Step3 to implement a Gibbs sampler that uses $\vec{\mu^0}=(0,...,0)$ as a starting point. Run the Gibbs sampler to obtain **1000** values of $\vec{\mu}$ and then compute the expected value of $\vec{\mu}$ by using a Montel Carlo approach.
```{r}
f.MCMC.Gibbs <- function(nstep,X0)



f.MCMC.Gibbs<-function(nstep,dim,Y,var){
        #nstep = number of mu values you want to obtain with Gibbs sampler
        #d = dimension
        mu <- matrix(0,nrow=nstep,ncol=dim)
        
        
        for (i in 2:nstep){
                post_mu[i,1] <- ?
                        
                for (d in 2:(dim-1)){
                        post_mu[i,d] <- exp(-(mu[i,j]-(Y[i]+mu[i-1,j])/2)^2/(var/2))
                        
                        
                }
                
                post_mu[dim,i] <- ?
        }
        return(post_mu)
}

    
    for (i in 2:nstep){
	X<-mX[i-1,]
	Y<-rep(0,d)
        Y[1]<-rnorm(1,
                    mean=vmean[1]+(mVar[1,-1]%*%solve(mVar[-1,-1]))%*%(X[2:d]-vmean[-1]),
                    sd=sqrt(mVar[1,1]-mVar[1,-1]%*%solve(mVar[-1,-1])%*%mVar[-1,1]))
        
	for (j in 2:(d-1)){
	    Y[j]<-rnorm(1,
	                mean=vmean[j]+(mVar[j,-j]%*%solve(mVar[-j,-j]))%*%(c(Y[1:(j-1)],X[(j+1):d])-vmean[-j]),
	                sd=sqrt(mVar[j,j]-mVar[j,-j]%*%solve(mVar[-j,-j])%*%mVar[-j,j]))
	}
        
	Y[d]<-rnorm(1,mean=vmean[d]+(mVar[d,-d]%*%solve(mVar[-d,-d]))%*%(Y[1:(d-1)]-vmean[-d]),
	            sd=sqrt(mVar[d,d]-mVar[d,-d]%*%solve(mVar[-d,-d])%*%mVar[-d,d]))
	mX[i,]<-Y
    }
    mX
}
```

```{r}
nstep <- 2000
dim <- nrow(data)
Y <-data$Y
sigma <- 0.4

f.MCMC.Gibbs()
```
#####Plot the expected value of $\vec{\mu}$ vs X and Y vs X in the same graph.

#####Q.Does it seem that you have managed to remove the noise?
#####Q.Does it seem that the expected value of $\vec{\mu}$ can catch the true underlying dependence between Y and X?












